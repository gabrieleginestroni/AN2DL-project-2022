{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"830302ffa9751c9d00ec7acc7b0cf1db10db4d450fa3f08a82573b165c9771b0"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import libraries","metadata":{"id":"zfehjCy896Fd"}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight, shuffle\nfrom tensorflow.keras import mixed_precision\nimport warnings\nimport logging\n\n# Enable mixed precision\nmixed_precision.set_global_policy('mixed_float16')\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nprint(tf.__version__)\n\nprint(tf.config.list_physical_devices())\n\nimport shutil\nfrom IPython.display import FileLink\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_wVYNVVfr6q","outputId":"812f6cdf-089b-4ef4-a5f6-7e69cdcfa205","execution":{"iopub.status.busy":"2022-12-18T22:11:07.303459Z","iopub.execute_input":"2022-12-18T22:11:07.303827Z","iopub.status.idle":"2022-12-18T22:11:07.315455Z","shell.execute_reply.started":"2022-12-18T22:11:07.303775Z","shell.execute_reply":"2022-12-18T22:11:07.314464Z"},"trusted":true},"execution_count":344,"outputs":[{"name":"stdout","text":"2.6.4\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set seed for reproducibility","metadata":{"id":"lQWEHP13tQsc"}},{"cell_type":"code","source":"# Random seed for reproducibility\nseed = 102\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"id":"imBtfhUPLwB-","execution":{"iopub.status.busy":"2022-12-18T22:11:07.320118Z","iopub.execute_input":"2022-12-18T22:11:07.320816Z","iopub.status.idle":"2022-12-18T22:11:07.359754Z","shell.execute_reply.started":"2022-12-18T22:11:07.320753Z","shell.execute_reply":"2022-12-18T22:11:07.358569Z"},"trusted":true},"execution_count":345,"outputs":[]},{"cell_type":"markdown","source":"Hyper parameters for augmentation, splines interpolation and scaling","metadata":{}},{"cell_type":"code","source":"scaling = True # applies standard scaling to the data\napply_oversampling = False \ninterpolation_multiplier = 3 # resolution multiplier\naugment_std = 0.04# introduces noise to the oversampled data\nuse_cross_valid = False","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.361606Z","iopub.execute_input":"2022-12-18T22:11:07.361977Z","iopub.status.idle":"2022-12-18T22:11:07.374604Z","shell.execute_reply.started":"2022-12-18T22:11:07.361944Z","shell.execute_reply":"2022-12-18T22:11:07.373646Z"},"trusted":true},"execution_count":346,"outputs":[]},{"cell_type":"markdown","source":" ## Load the training dataset","metadata":{"id":"GMysAd_I-lED"}},{"cell_type":"code","source":"path = \"/kaggle/input/training/\"\n# Load the .npy file\nX = np.load(path + \"x_train.npy\")\ny = np.load(path + \"y_train.npy\")\n\n# Convert to float32 for less precision and better performance\nX = X.astype(np.float32)\ny = y.astype(np.int32)\n\n# shuffle X, y\nX, y = shuffle(X, y, random_state=seed)\n\nprint(X.shape, X.dtype, sep=\", \")   \nprint(y.shape, y.dtype, sep=\", \") ","metadata":{"id":"C0m6Gn4fkm9S","execution":{"iopub.status.busy":"2022-12-18T22:11:07.375880Z","iopub.execute_input":"2022-12-18T22:11:07.376655Z","iopub.status.idle":"2022-12-18T22:11:07.405253Z","shell.execute_reply.started":"2022-12-18T22:11:07.376618Z","shell.execute_reply":"2022-12-18T22:11:07.404364Z"},"trusted":true},"execution_count":347,"outputs":[{"name":"stdout","text":"(2429, 36, 6), float32\n(2429,), int32\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Training - Validation Split","metadata":{}},{"cell_type":"code","source":"if not use_cross_valid:\n    # Split the data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15,shuffle=True, random_state=seed, stratify=y)\n    print(\"training set : \", X_train.shape)\n    print(\"test set: \", X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.407656Z","iopub.execute_input":"2022-12-18T22:11:07.408264Z","iopub.status.idle":"2022-12-18T22:11:07.418760Z","shell.execute_reply.started":"2022-12-18T22:11:07.408228Z","shell.execute_reply":"2022-12-18T22:11:07.417694Z"},"trusted":true},"execution_count":348,"outputs":[{"name":"stdout","text":"training set :  (2064, 36, 6)\ntest set:  (365, 36, 6)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Plotting the classes distributions","metadata":{}},{"cell_type":"code","source":"# Map classes STRINGS to integers\nlabel_mapping = {\n    'Wish': 0,\n    'Another': 1,\n    'Comfortably': 2,\n    'Money': 3,\n    'Breathe': 4,\n    'Time': 5,\n    'Brain': 6,\n    'Echoes': 7,\n    'Wearing': 8,\n    'Sorrow': 9,\n    'Hey': 10,\n    'Shine': 11,  \n}\n\n#vertical bar plot of the classes distribution in y\nplt.title('Classes distribution')\nplt.bar(label_mapping.keys(), np.bincount(y), color = matplotlib.colormaps['Set2'].colors)\nplt.xticks(rotation=90)\nplt.ylabel('Count')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.420319Z","iopub.execute_input":"2022-12-18T22:11:07.421400Z","iopub.status.idle":"2022-12-18T22:11:07.652626Z","shell.execute_reply.started":"2022-12-18T22:11:07.421365Z","shell.execute_reply":"2022-12-18T22:11:07.651670Z"},"trusted":true},"execution_count":349,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Class weights computation","metadata":{}},{"cell_type":"code","source":"# Compute the class weights in order to balance loss during training\nlabels = np.unique(np.fromiter([t for t in y], np.int32))\nclass_weights = dict(enumerate(class_weight.compute_class_weight('balanced', classes=labels, y=y)))\n\n# plot class weights with respect to the labels\nplt.title('Class weights')\nplt.bar(label_mapping.keys(), class_weights.values(), color = matplotlib.colormaps['Set2'].colors)\nplt.xticks(rotation=90)\nplt.ylabel('Weight')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.653967Z","iopub.execute_input":"2022-12-18T22:11:07.654715Z","iopub.status.idle":"2022-12-18T22:11:07.876819Z","shell.execute_reply.started":"2022-12-18T22:11:07.654676Z","shell.execute_reply":"2022-12-18T22:11:07.875897Z"},"trusted":true},"execution_count":350,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## Data preprocessing with cubic interpolation between data points for increasing resolution","metadata":{}},{"cell_type":"markdown","source":"Before the cubic spline interpolation ","metadata":{}},{"cell_type":"code","source":"def spline_augment_print(X_train,y_train,interpolation_multiplier = 3,augment_std = 0.02):\n    # plot the sequence of points in a randomly drawn sample from X\n    sample = random.randint(0, X_train.shape[0])\n\n    # create a figure with 6 subplots, one plot per each feature\n    fig, axs = plt.subplots(6, 1, figsize=(9, 13))\n    fig.suptitle('Sample ' + str(sample))\n    for i in range(6):\n        axs[i].set_title('Feature ' + str(i))\n        # add a dot at each point in the sequence\n        axs[i].plot(X_train[sample, :, i], 'o-', color='blue', markersize=4)\n\n    plt.show()\n    \n    from scipy import interpolate\n\n    # triple the number of points in the sequence to increase the resolution by a factor of 3\n    x_lin = np.linspace(0, 35, 36 * interpolation_multiplier)\n\n    # new augmented dataset\n    X_spline = np.zeros((X_train.shape[0], x_lin.size, 6))\n    print(X_spline.shape)\n\n    for i in np.arange(X_train[:, 0, 0].size): # for each sample\n        for j in np.arange(6): # for each feature\n            # add a cubic spline interpolation between the data points of X[sample, :, 0]\n            interpolation = interpolate.interp1d(np.arange(0, 36), X_train[i, :, j], kind='cubic', fill_value=\"extrapolate\")\n            X_spline[i, :, j] = interpolation(x_lin)\n    # create a figure with 6 subplots, one plot per each feature\n    fig, axs = plt.subplots(6, 1, figsize=(9, 14))\n    fig.suptitle('Sample ' + str(sample))\n    for i in range(6):\n        axs[i].set_title('Feature ' + str(i))\n        # plots the given sequence of points without interpolation\n        axs[i].plot(np.arange(0, 36), X_train[sample, :, i], 'o-', color='blue', markersize=4)\n\n        # plots the new set of points with cubic spline interpolation\n        axs[i].plot(x_lin, X_spline[sample, :, i], \"o-\", color='red', markersize = 3, alpha = 0.7)\n    plt.show()\n    # create a new augmented dataset with the new samples that is as big as the original training dataset\n    X_augmented = np.empty((0, X_train.shape[1], X_train.shape[2]), dtype=np.float32)\n    y_augmented = np.empty((0,), dtype=np.int32)\n\n    print(\"interpolation_multiplier = \", interpolation_multiplier)\n\n    # for each sample in the training dataset\n    for i in range(y_train.shape[0]):\n\n        # copy the original sample in the augmented dataset every 3 points from X_spline[i, k, :]\n        for j in range(interpolation_multiplier):\n            X_augmented = np.append(X_augmented, np.expand_dims(X_spline[i, j::interpolation_multiplier, :], axis=0), axis=0)\n            y_augmented = np.append(y_augmented, np.expand_dims(y_train[i], axis=0), axis=0)\n\n            # for each sample, add a random gaussian noise to each feature\n\n            for k in range(X_augmented.shape[2]):\n                mins = np.min(X_augmented[-1, :, k], axis=0)\n                maxs = np.max(X_augmented[-1, :, k], axis=0)\n\n                values = np.random.normal(0, augment_std * (maxs - mins), X_augmented.shape[1])\n                X_augmented[-1, :, k] = np.add(X_augmented[-1, :, k], values)\n\n    print(\"new augmented dataset size \", X_augmented.shape, y_augmented.shape, sep=\", \")\n\n    # shuffle the data\n    X_augmented, y_augmented = shuffle(X_augmented, y_augmented, random_state=seed)\n    \n    return X_augmented, y_augmented","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.879695Z","iopub.execute_input":"2022-12-18T22:11:07.879985Z","iopub.status.idle":"2022-12-18T22:11:07.897143Z","shell.execute_reply.started":"2022-12-18T22:11:07.879959Z","shell.execute_reply":"2022-12-18T22:11:07.896159Z"},"trusted":true},"execution_count":351,"outputs":[]},{"cell_type":"code","source":"#redefining the function not to print (to be used in crossvalidation)\ndef spline_augment_no_print(X_train,y_train,interpolation_multiplier = 3,augment_std = 0.02):\n\n    from scipy import interpolate\n\n    # triple the number of points in the sequence to increase the resolution by a factor of 3\n    x_lin = np.linspace(0, 35, 36 * interpolation_multiplier)\n\n    # new augmented dataset\n    X_spline = np.zeros((X_train.shape[0], x_lin.size, 6))\n\n\n    for i in np.arange(X_train[:, 0, 0].size): # for each sample\n        for j in np.arange(6): # for each feature\n            # add a cubic spline interpolation between the data points of X[sample, :, 0]\n            interpolation = interpolate.interp1d(np.arange(0, 36), X_train[i, :, j], kind='cubic', fill_value=\"extrapolate\")\n            X_spline[i, :, j] = interpolation(x_lin)\n    \n    # create a new augmented dataset with the new samples that is as big as the original training dataset\n    X_augmented = np.empty((0, X_train.shape[1], X_train.shape[2]), dtype=np.float32)\n    y_augmented = np.empty((0,), dtype=np.int32)\n\n    # for each sample in the training dataset\n    for i in range(y_train.shape[0]):\n\n        # copy the original sample in the augmented dataset every 3 points from X_spline[i, k, :]\n        for j in range(interpolation_multiplier):\n            X_augmented = np.append(X_augmented, np.expand_dims(X_spline[i, j::interpolation_multiplier, :], axis=0), axis=0)\n            y_augmented = np.append(y_augmented, np.expand_dims(y_train[i], axis=0), axis=0)\n\n            # for each sample, add a random gaussian noise to each feature\n\n            for k in range(X_augmented.shape[2]):\n                mins = np.min(X_augmented[-1, :, k], axis=0)\n                maxs = np.max(X_augmented[-1, :, k], axis=0)\n\n                values = np.random.normal(0, augment_std * (maxs - mins), X_augmented.shape[1])\n                X_augmented[-1, :, k] = np.add(X_augmented[-1, :, k], values)\n\n    # shuffle the data\n    X_augmented, y_augmented = shuffle(X_augmented, y_augmented, random_state=seed)\n    \n    return X_augmented, y_augmented","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.899475Z","iopub.execute_input":"2022-12-18T22:11:07.899862Z","iopub.status.idle":"2022-12-18T22:11:07.913685Z","shell.execute_reply.started":"2022-12-18T22:11:07.899826Z","shell.execute_reply":"2022-12-18T22:11:07.912696Z"},"trusted":true},"execution_count":352,"outputs":[]},{"cell_type":"markdown","source":"## Application of Standard Scaler to the dataset","metadata":{}},{"cell_type":"code","source":"if not use_cross_valid:\n    #X_train = np.delete(X_train, 3, axis=2)\n    #X_test = np.delete(X_test, 3, axis=2)\n    \n    features_num = X_train.shape[2]\n    print(\"features: \", features_num)\n\n    #(optional) apply spline augmentation\n    #X_train,y_train = spline_augment_print(X_train,y_train,interpolation_multiplier = 3,augment_std = 0.02)\n    # if the standard scaling is requested, computes it on the training set and prints its values\n    if (scaling):\n        scaler_robust = RobustScaler()\n        scaler_std = StandardScaler()\n        scaler_0 = MinMaxScaler(feature_range=(0,1))\n        #scaler_1 = MinMaxScaler(feature_range=(-1,1))\n\n        num_instances, num_time_steps, num_features = X_train.shape\n        X_train = np.reshape(X_train, newshape=(-1, num_features))\n        X_train = scaler_robust.fit_transform(X_train)\n        X_train = scaler_0.fit_transform(X_train)\n        X_train = scaler_std.fit_transform(X_train)\n\n        X_train = np.reshape(X_train, newshape=(num_instances, num_time_steps, num_features))\n\n        num_instances, num_time_steps, num_features = X_test.shape\n        X_test = np.reshape(X_test, newshape=(-1, num_features))\n        X_test = scaler_robust.transform(X_test)\n        X_test = scaler_0.transform(X_test)\n        X_test = scaler_std.transform(X_test)\n\n        X_test = np.reshape(X_test, newshape=(num_instances, num_time_steps, num_features))\n        #print(\"std_scaler: \"+str(scaler_std.__dict__))\n        #print(\"min_max_scaler_1: \"+str(scaler_1.__dict__))\n        #print(\"min_max_scaler_0: \"+str(scaler_0.__dict__))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.917032Z","iopub.execute_input":"2022-12-18T22:11:07.917325Z","iopub.status.idle":"2022-12-18T22:11:07.974890Z","shell.execute_reply.started":"2022-12-18T22:11:07.917298Z","shell.execute_reply":"2022-12-18T22:11:07.973819Z"},"trusted":true},"execution_count":353,"outputs":[{"name":"stdout","text":"features:  6\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Categorical labels for the training and test data","metadata":{}},{"cell_type":"code","source":"if not use_cross_valid:   \n    # Convert the sparse labels to categorical values\n    y_train = tfk.utils.to_categorical(y_train)\n    y_test = tfk.utils.to_categorical(y_test)\n    print(X_train.shape, y_train.shape, sep=\", \")\n    print(X_test.shape, y_test.shape, sep=\", \")","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.976453Z","iopub.execute_input":"2022-12-18T22:11:07.976926Z","iopub.status.idle":"2022-12-18T22:11:07.985647Z","shell.execute_reply.started":"2022-12-18T22:11:07.976886Z","shell.execute_reply":"2022-12-18T22:11:07.984348Z"},"trusted":true},"execution_count":354,"outputs":[{"name":"stdout","text":"(2064, 36, 6), (2064, 12)\n(365, 36, 6), (365, 12)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Build the models","metadata":{}},{"cell_type":"markdown","source":"Hyperparameters","metadata":{}},{"cell_type":"code","source":"if not use_cross_valid:  \n    input_shape = X_train.shape[1:]\n    print(\"input shape: \", input_shape)\n    classes = y_train.shape[-1]\nbatch_size = 512\nepochs = 1000\n","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.987003Z","iopub.execute_input":"2022-12-18T22:11:07.987442Z","iopub.status.idle":"2022-12-18T22:11:07.996949Z","shell.execute_reply.started":"2022-12-18T22:11:07.987406Z","shell.execute_reply":"2022-12-18T22:11:07.995815Z"},"trusted":true},"execution_count":355,"outputs":[{"name":"stdout","text":"input shape:  (36, 6)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Plot history of the training and the learning rate schedule","metadata":{}},{"cell_type":"code","source":"def plot_history(history):\n\tbest_epoch = np.argmax(history['val_accuracy'])\n\tplt.figure(figsize=(17,4))\n\tplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n\tplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n\tplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n\tplt.title('Categorical Crossentropy')\n\tplt.legend()\n\tplt.grid(alpha=.3)\n\tplt.show()\n\n\tplt.figure(figsize=(17,4))\n\tplt.plot(history['accuracy'], label='Training accuracy', alpha=.8, color='#ff7f0e')\n\tplt.plot(history['val_accuracy'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\n\tplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n\tplt.title('Accuracy')\n\tplt.legend()\n\tplt.grid(alpha=.3)\n\tplt.show()\n\n\tplt.figure(figsize=(17,4))\n\tplt.title('Learning Rate Schedule')\n\tplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n\tplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n\tplt.legend()\n\tplt.grid(alpha=.3)\n\tplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:07.998336Z","iopub.execute_input":"2022-12-18T22:11:07.998931Z","iopub.status.idle":"2022-12-18T22:11:08.010418Z","shell.execute_reply.started":"2022-12-18T22:11:07.998874Z","shell.execute_reply":"2022-12-18T22:11:08.008971Z"},"trusted":true},"execution_count":356,"outputs":[]},{"cell_type":"markdown","source":"Plot confusion matrix, F1 scores and displays model accuracy","metadata":{}},{"cell_type":"code","source":"def plot_statistics(predictions):\t\n\t# Compute the confusion matrix\n\tcm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n\n\t# Compute the classification metrics\n\taccuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n\tprecision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n\trecall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n\tf1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n\tprint('Accuracy:',accuracy.round(4))\n\tprint('Precision:',precision.round(4))\n\tprint('Recall:',recall.round(4))\n\tprint('F1:',f1.round(4))\n\n\t# Plot the confusion matrix\n\tplt.figure(figsize=(7, 6))\n\tplt.title('Confusion matrix')\n\tsns.heatmap(cm.T, cmap='Blues', xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()), annot=True, fmt=\"d\")\n\tplt.xlabel('True labels')\n\tplt.ylabel('Predicted labels')\n\tplt.show()\n\n\t# horizontal bar plot of the f1 scores for each class\n\tplt.figure(figsize=(6,4))\n\tplt.grid(alpha=.3)\n\tplt.barh(y=list(label_mapping.keys()), width=f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average=None))\n\tplt.title('F1 scores')\n\tplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:08.013136Z","iopub.execute_input":"2022-12-18T22:11:08.013597Z","iopub.status.idle":"2022-12-18T22:11:08.025149Z","shell.execute_reply.started":"2022-12-18T22:11:08.013561Z","shell.execute_reply":"2022-12-18T22:11:08.024076Z"},"trusted":true},"execution_count":357,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\nclass AttentionLayer(tfkl.Layer):\n    def _init_(self, **kwargs):\n        super(AttentionLayer, self)._init_(**kwargs)\n\n    def build(self, input_shape):\n        # create a trainable weight variable for this layer which is used to compute a score for each time step in the input sequence\n        self.W = self.add_weight(name='attention_weight', \n                                 shape=(input_shape[-1], 1),\n                                 initializer='uniform',\n                                 trainable=True)\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, inputs, mask=None):\n        # inputs: (batch_size, time_steps, input_dim)\n        # mask: (batch_size, time_steps)\n\n        # apply a dot product between the input and the trainable weight to\n        # get a score for each time step\n        score = K.dot(inputs, self.W)\n        score = K.squeeze(score, axis=-1)\n        # apply a softmax activation to the score to compute the attention\n        # weights for each time step\n        attention_weights = K.softmax(score)\n        # multiply the attention weights by the input to compute the\n        # weighted sum of the input\n        weighted_sum = K.sum(K.expand_dims(attention_weights, axis=-1) * inputs, axis=1)\n        # return the attention-weighted sum of the input\n        return weighted_sum\n\n    def compute_output_shape(self, input_shape):\n        # the output shape is the same as the input shape, except the input\n        # dimension is reduced by one\n        return (input_shape[0], input_shape[-1])","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:08.030269Z","iopub.execute_input":"2022-12-18T22:11:08.030680Z","iopub.status.idle":"2022-12-18T22:11:08.040571Z","shell.execute_reply.started":"2022-12-18T22:11:08.030645Z","shell.execute_reply":"2022-12-18T22:11:08.039515Z"},"trusted":true},"execution_count":358,"outputs":[]},{"cell_type":"markdown","source":"#### Only convolutional and dense layers","metadata":{}},{"cell_type":"code","source":"\n\ndef build_classifier(input_shape, classes):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    \n    #head1\n    y = tfkl.Conv1D(\n        filters = 256,\n        kernel_size = 3,\n        padding = 'same',\n        activation = 'relu',\n        kernel_initializer = tfk.initializers.HeUniform(seed),\n        name = 'conv1y')(input_layer)\n\n    #y = tfkl.Dropout(0.2, seed=seed)(y)\n\n    y = tfkl.Conv1D(\n        filters = 256,\n        kernel_size = 3,\n        padding = 'same',\n        activation = 'relu',\n        kernel_initializer = tfk.initializers.HeUniform(seed),\n        name = 'conv2y')(y)\n    \n    #y = tfkl.Dropout(0.2, seed=seed)(y)\n    \n    y = tfkl.Conv1D(\n        filters = 128,\n        kernel_size = 3,\n        padding = 'same',\n        activation = 'relu',\n        kernel_initializer = tfk.initializers.HeUniform(seed),\n        name = 'conv3y')(y)\n\n    #y = tfkl.Dropout(0.2, seed=seed)(y)\n    \n\n    #head4-LSTM\n    \n    r = tfkl.Bidirectional(tfkl.GRU(60, return_sequences=True, name='lstm1'))(y)\n    r = tfkl.Dropout(0.5, seed=seed)(r)\n    r = tfkl.Bidirectional(tfkl.GRU(60, return_sequences=True, name='lstm2'))(r)\n    r = tfkl.Dropout(0.5, seed=seed)(r)\n\n    #x = tfkl.Bidirectional(tfkl.GRU(16, return_sequences=True, name='lstm3'))(y)\n    #x = tfkl.Dropout(0.4, seed=seed)(x)\n    #x = tfkl.Bidirectional(tfkl.GRU(16, return_sequences=False, name='lstm2'))(x)\n    #x = tfkl.Dropout(0.4, seed=seed)(x)\n\n    #x = tfkl.GlobalAveragePooling1D()(r)\n    #x = tfkl.Dropout(0.3, seed=seed)(x)\n    x = AttentionLayer()(r)\n\n    #x = tfkl.Dropout(0.3, seed=seed)(x)\n    output_layer = tfkl.Dense(classes, activation='softmax',kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n\n    # Connect input and output through the Model class\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model1')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(3e-3), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:08.042737Z","iopub.execute_input":"2022-12-18T22:11:08.043155Z","iopub.status.idle":"2022-12-18T22:11:08.055547Z","shell.execute_reply.started":"2022-12-18T22:11:08.043118Z","shell.execute_reply":"2022-12-18T22:11:08.054541Z"},"trusted":true},"execution_count":359,"outputs":[]},{"cell_type":"code","source":"if not use_cross_valid:  \n    model = build_classifier(input_shape,classes)\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:08.057180Z","iopub.execute_input":"2022-12-18T22:11:08.057541Z","iopub.status.idle":"2022-12-18T22:11:09.013879Z","shell.execute_reply.started":"2022-12-18T22:11:08.057503Z","shell.execute_reply":"2022-12-18T22:11:09.012852Z"},"trusted":true},"execution_count":360,"outputs":[{"name":"stdout","text":"Model: \"model1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nInput (InputLayer)           [(None, 36, 6)]           0         \n_________________________________________________________________\nconv1y (Conv1D)              (None, 36, 256)           4864      \n_________________________________________________________________\nconv2y (Conv1D)              (None, 36, 256)           196864    \n_________________________________________________________________\nconv3y (Conv1D)              (None, 36, 128)           98432     \n_________________________________________________________________\nbidirectional_62 (Bidirectio (None, 36, 120)           68400     \n_________________________________________________________________\ndropout_68 (Dropout)         (None, 36, 120)           0         \n_________________________________________________________________\nbidirectional_63 (Bidirectio (None, 36, 120)           65520     \n_________________________________________________________________\ndropout_69 (Dropout)         (None, 36, 120)           0         \n_________________________________________________________________\nattention_layer_31 (Attentio (None, 120)               120       \n_________________________________________________________________\ndense_31 (Dense)             (None, 12)                1452      \n=================================================================\nTotal params: 435,652\nTrainable params: 435,652\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"if not use_cross_valid:\n    \n    # Train the model\n    history = model.fit(\n        x = X_train,\n        y = y_train,\n        batch_size = batch_size,\n        epochs = epochs,\n        validation_data = (X_test, y_test),\n        #class_weight = class_weights,\n        callbacks = [\n            tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=150, restore_best_weights=True),\n            #tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=7, factor=tf.math.exp(-0.1), min_lr=5e-5)\n            tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=20, factor=0.4, min_lr=5e-5)\n        ]\n    ).history","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:11:09.015574Z","iopub.execute_input":"2022-12-18T22:11:09.016241Z","iopub.status.idle":"2022-12-18T22:12:02.368071Z","shell.execute_reply.started":"2022-12-18T22:11:09.016202Z","shell.execute_reply":"2022-12-18T22:12:02.367119Z"},"trusted":true},"execution_count":361,"outputs":[{"name":"stdout","text":"Epoch 1/1000\n5/5 [==============================] - 7s 294ms/step - loss: 2.2049 - accuracy: 0.2752 - val_loss: 2.0605 - val_accuracy: 0.1890\nEpoch 2/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.9162 - accuracy: 0.3358 - val_loss: 1.8262 - val_accuracy: 0.3973\nEpoch 3/1000\n5/5 [==============================] - 0s 39ms/step - loss: 1.7499 - accuracy: 0.4191 - val_loss: 1.7607 - val_accuracy: 0.4247\nEpoch 4/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.6466 - accuracy: 0.4419 - val_loss: 1.7275 - val_accuracy: 0.4192\nEpoch 5/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.6374 - accuracy: 0.4385 - val_loss: 1.6152 - val_accuracy: 0.4329\nEpoch 6/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.5061 - accuracy: 0.4782 - val_loss: 1.5342 - val_accuracy: 0.4740\nEpoch 7/1000\n5/5 [==============================] - 0s 39ms/step - loss: 1.4260 - accuracy: 0.5015 - val_loss: 1.4756 - val_accuracy: 0.4959\nEpoch 8/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.3821 - accuracy: 0.5165 - val_loss: 1.4746 - val_accuracy: 0.4986\nEpoch 9/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.3349 - accuracy: 0.5252 - val_loss: 1.3711 - val_accuracy: 0.5288\nEpoch 10/1000\n5/5 [==============================] - 0s 38ms/step - loss: 1.2943 - accuracy: 0.5373 - val_loss: 1.3740 - val_accuracy: 0.5315\nEpoch 11/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.2805 - accuracy: 0.5591 - val_loss: 1.3838 - val_accuracy: 0.5014\nEpoch 12/1000\n5/5 [==============================] - 0s 37ms/step - loss: 1.3092 - accuracy: 0.5339 - val_loss: 1.3008 - val_accuracy: 0.5370\nEpoch 13/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.2276 - accuracy: 0.5596 - val_loss: 1.3105 - val_accuracy: 0.5233\nEpoch 14/1000\n5/5 [==============================] - 0s 37ms/step - loss: 1.2170 - accuracy: 0.5780 - val_loss: 1.3037 - val_accuracy: 0.5890\nEpoch 15/1000\n5/5 [==============================] - 0s 38ms/step - loss: 1.2210 - accuracy: 0.5785 - val_loss: 1.2441 - val_accuracy: 0.5616\nEpoch 16/1000\n5/5 [==============================] - 0s 39ms/step - loss: 1.1538 - accuracy: 0.6056 - val_loss: 1.3135 - val_accuracy: 0.5342\nEpoch 17/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.2082 - accuracy: 0.5727 - val_loss: 1.2451 - val_accuracy: 0.5507\nEpoch 18/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.1682 - accuracy: 0.5959 - val_loss: 1.3555 - val_accuracy: 0.5589\nEpoch 19/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.1785 - accuracy: 0.5761 - val_loss: 1.1768 - val_accuracy: 0.5753\nEpoch 20/1000\n5/5 [==============================] - 0s 49ms/step - loss: 1.1229 - accuracy: 0.6148 - val_loss: 1.2598 - val_accuracy: 0.5534\nEpoch 21/1000\n5/5 [==============================] - 0s 41ms/step - loss: 1.1583 - accuracy: 0.5891 - val_loss: 1.3018 - val_accuracy: 0.5370\nEpoch 22/1000\n5/5 [==============================] - 0s 42ms/step - loss: 1.1357 - accuracy: 0.5998 - val_loss: 1.2236 - val_accuracy: 0.5753\nEpoch 23/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.0717 - accuracy: 0.6231 - val_loss: 1.2080 - val_accuracy: 0.5699\nEpoch 24/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.0629 - accuracy: 0.6211 - val_loss: 1.1670 - val_accuracy: 0.6055\nEpoch 25/1000\n5/5 [==============================] - 0s 36ms/step - loss: 1.0209 - accuracy: 0.6420 - val_loss: 1.0996 - val_accuracy: 0.6082\nEpoch 26/1000\n5/5 [==============================] - 0s 37ms/step - loss: 1.0107 - accuracy: 0.6536 - val_loss: 1.0762 - val_accuracy: 0.6274\nEpoch 27/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.9970 - accuracy: 0.6662 - val_loss: 1.0811 - val_accuracy: 0.6356\nEpoch 28/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.9844 - accuracy: 0.6710 - val_loss: 1.0293 - val_accuracy: 0.6301\nEpoch 29/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.9737 - accuracy: 0.6667 - val_loss: 1.0811 - val_accuracy: 0.6219\nEpoch 30/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.9373 - accuracy: 0.6686 - val_loss: 1.0801 - val_accuracy: 0.6329\nEpoch 31/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.9172 - accuracy: 0.6846 - val_loss: 1.0635 - val_accuracy: 0.6438\nEpoch 32/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.9197 - accuracy: 0.6797 - val_loss: 1.0254 - val_accuracy: 0.6603\nEpoch 33/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.9012 - accuracy: 0.6957 - val_loss: 0.9941 - val_accuracy: 0.6658\nEpoch 34/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.9121 - accuracy: 0.6899 - val_loss: 0.9888 - val_accuracy: 0.6603\nEpoch 35/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.9203 - accuracy: 0.6749 - val_loss: 1.0352 - val_accuracy: 0.6384\nEpoch 36/1000\n5/5 [==============================] - 0s 40ms/step - loss: 0.9241 - accuracy: 0.6773 - val_loss: 0.9927 - val_accuracy: 0.6575\nEpoch 37/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.9006 - accuracy: 0.6831 - val_loss: 1.0449 - val_accuracy: 0.6466\nEpoch 38/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.8995 - accuracy: 0.6822 - val_loss: 1.0078 - val_accuracy: 0.6521\nEpoch 39/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.8846 - accuracy: 0.6904 - val_loss: 1.0137 - val_accuracy: 0.6658\nEpoch 40/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.8798 - accuracy: 0.6962 - val_loss: 0.9805 - val_accuracy: 0.6767\nEpoch 41/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.8760 - accuracy: 0.6982 - val_loss: 0.9399 - val_accuracy: 0.6877\nEpoch 42/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.8290 - accuracy: 0.7185 - val_loss: 0.9590 - val_accuracy: 0.6658\nEpoch 43/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.7977 - accuracy: 0.7272 - val_loss: 0.9800 - val_accuracy: 0.6466\nEpoch 44/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.8072 - accuracy: 0.7185 - val_loss: 1.0693 - val_accuracy: 0.6548\nEpoch 45/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.9252 - accuracy: 0.6778 - val_loss: 1.0312 - val_accuracy: 0.6493\nEpoch 46/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.9489 - accuracy: 0.6584 - val_loss: 1.0693 - val_accuracy: 0.6603\nEpoch 47/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.8700 - accuracy: 0.7049 - val_loss: 1.0928 - val_accuracy: 0.6493\nEpoch 48/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.8885 - accuracy: 0.6870 - val_loss: 1.0430 - val_accuracy: 0.6548\nEpoch 49/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.8451 - accuracy: 0.7054 - val_loss: 1.0078 - val_accuracy: 0.6493\nEpoch 50/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.8152 - accuracy: 0.7238 - val_loss: 0.9912 - val_accuracy: 0.6630\nEpoch 51/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.8067 - accuracy: 0.7301 - val_loss: 0.9888 - val_accuracy: 0.6493\nEpoch 52/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.7710 - accuracy: 0.7418 - val_loss: 0.9731 - val_accuracy: 0.6630\nEpoch 53/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.7687 - accuracy: 0.7374 - val_loss: 0.9995 - val_accuracy: 0.6247\nEpoch 54/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.7638 - accuracy: 0.7287 - val_loss: 0.9868 - val_accuracy: 0.6822\nEpoch 55/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.7506 - accuracy: 0.7393 - val_loss: 1.0654 - val_accuracy: 0.6548\nEpoch 56/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.7686 - accuracy: 0.7427 - val_loss: 1.0273 - val_accuracy: 0.6575\nEpoch 57/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.7420 - accuracy: 0.7510 - val_loss: 0.9624 - val_accuracy: 0.6932\nEpoch 58/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6883 - accuracy: 0.7641 - val_loss: 0.9761 - val_accuracy: 0.6767\nEpoch 59/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.6853 - accuracy: 0.7578 - val_loss: 0.9580 - val_accuracy: 0.6932\nEpoch 60/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6917 - accuracy: 0.7616 - val_loss: 0.9883 - val_accuracy: 0.6877\nEpoch 61/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6968 - accuracy: 0.7650 - val_loss: 0.9736 - val_accuracy: 0.6877\nEpoch 62/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.7105 - accuracy: 0.7505 - val_loss: 1.0264 - val_accuracy: 0.6521\nEpoch 63/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.7077 - accuracy: 0.7539 - val_loss: 0.9434 - val_accuracy: 0.6904\nEpoch 64/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6997 - accuracy: 0.7587 - val_loss: 1.0049 - val_accuracy: 0.6712\nEpoch 65/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.7028 - accuracy: 0.7490 - val_loss: 0.9927 - val_accuracy: 0.6959\nEpoch 66/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6651 - accuracy: 0.7660 - val_loss: 0.9688 - val_accuracy: 0.6877\nEpoch 67/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6918 - accuracy: 0.7655 - val_loss: 1.0830 - val_accuracy: 0.6603\nEpoch 68/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6959 - accuracy: 0.7636 - val_loss: 1.0371 - val_accuracy: 0.6685\nEpoch 69/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6799 - accuracy: 0.7670 - val_loss: 0.9609 - val_accuracy: 0.6795\nEpoch 70/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.7182 - accuracy: 0.7558 - val_loss: 1.0176 - val_accuracy: 0.6630\nEpoch 71/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6692 - accuracy: 0.7694 - val_loss: 0.9229 - val_accuracy: 0.6877\nEpoch 72/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.6191 - accuracy: 0.7936 - val_loss: 0.9224 - val_accuracy: 0.7096\nEpoch 73/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.5813 - accuracy: 0.8047 - val_loss: 0.9351 - val_accuracy: 0.7205\nEpoch 74/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.6002 - accuracy: 0.7888 - val_loss: 0.9414 - val_accuracy: 0.6959\nEpoch 75/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6587 - accuracy: 0.7670 - val_loss: 0.9712 - val_accuracy: 0.6904\nEpoch 76/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.7044 - accuracy: 0.7573 - val_loss: 0.9819 - val_accuracy: 0.6822\nEpoch 77/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.6892 - accuracy: 0.7592 - val_loss: 1.0723 - val_accuracy: 0.6712\nEpoch 78/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.7318 - accuracy: 0.7418 - val_loss: 0.9370 - val_accuracy: 0.6795\nEpoch 79/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.6481 - accuracy: 0.7849 - val_loss: 0.9448 - val_accuracy: 0.6904\nEpoch 80/1000\n5/5 [==============================] - 0s 45ms/step - loss: 0.6312 - accuracy: 0.7815 - val_loss: 1.0166 - val_accuracy: 0.6822\nEpoch 81/1000\n5/5 [==============================] - 0s 40ms/step - loss: 0.6621 - accuracy: 0.7757 - val_loss: 1.0322 - val_accuracy: 0.6795\nEpoch 82/1000\n5/5 [==============================] - 0s 40ms/step - loss: 0.6470 - accuracy: 0.7718 - val_loss: 0.9937 - val_accuracy: 0.6986\nEpoch 83/1000\n5/5 [==============================] - 0s 39ms/step - loss: 0.6316 - accuracy: 0.7825 - val_loss: 0.9688 - val_accuracy: 0.7096\nEpoch 84/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.5709 - accuracy: 0.8149 - val_loss: 0.9165 - val_accuracy: 0.7041\nEpoch 85/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.5099 - accuracy: 0.8338 - val_loss: 0.9541 - val_accuracy: 0.7096\nEpoch 86/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.5261 - accuracy: 0.8275 - val_loss: 0.9780 - val_accuracy: 0.7151\nEpoch 87/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.5297 - accuracy: 0.8144 - val_loss: 1.0078 - val_accuracy: 0.6932\nEpoch 88/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.4944 - accuracy: 0.8299 - val_loss: 0.9609 - val_accuracy: 0.7123\nEpoch 89/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.5473 - accuracy: 0.8047 - val_loss: 0.9697 - val_accuracy: 0.7068\nEpoch 90/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.5101 - accuracy: 0.8222 - val_loss: 0.9565 - val_accuracy: 0.7123\nEpoch 91/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.4719 - accuracy: 0.8382 - val_loss: 1.0254 - val_accuracy: 0.6986\nEpoch 92/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.4996 - accuracy: 0.8328 - val_loss: 1.0449 - val_accuracy: 0.6932\nEpoch 93/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.4892 - accuracy: 0.8333 - val_loss: 0.9785 - val_accuracy: 0.7096\nEpoch 94/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.4650 - accuracy: 0.8474 - val_loss: 0.9287 - val_accuracy: 0.7178\nEpoch 95/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.4117 - accuracy: 0.8677 - val_loss: 0.9146 - val_accuracy: 0.6986\nEpoch 96/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.3929 - accuracy: 0.8721 - val_loss: 0.8955 - val_accuracy: 0.7068\nEpoch 97/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3989 - accuracy: 0.8702 - val_loss: 0.9053 - val_accuracy: 0.7178\nEpoch 98/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3635 - accuracy: 0.8871 - val_loss: 0.9785 - val_accuracy: 0.7260\nEpoch 99/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3589 - accuracy: 0.8842 - val_loss: 0.9854 - val_accuracy: 0.6986\nEpoch 100/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3698 - accuracy: 0.8842 - val_loss: 0.9556 - val_accuracy: 0.7233\nEpoch 101/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3324 - accuracy: 0.8900 - val_loss: 0.9380 - val_accuracy: 0.7370\nEpoch 102/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.3450 - accuracy: 0.8963 - val_loss: 0.9263 - val_accuracy: 0.7479\nEpoch 103/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3271 - accuracy: 0.8983 - val_loss: 0.9258 - val_accuracy: 0.7370\nEpoch 104/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3363 - accuracy: 0.8905 - val_loss: 0.9478 - val_accuracy: 0.7315\nEpoch 105/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3359 - accuracy: 0.8944 - val_loss: 0.9854 - val_accuracy: 0.7288\nEpoch 106/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3262 - accuracy: 0.8944 - val_loss: 0.9688 - val_accuracy: 0.7205\nEpoch 107/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3591 - accuracy: 0.8871 - val_loss: 0.9795 - val_accuracy: 0.7123\nEpoch 108/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3078 - accuracy: 0.9007 - val_loss: 1.0195 - val_accuracy: 0.7260\nEpoch 109/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.3043 - accuracy: 0.9016 - val_loss: 0.9966 - val_accuracy: 0.7315\nEpoch 110/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2865 - accuracy: 0.9104 - val_loss: 0.9736 - val_accuracy: 0.7288\nEpoch 111/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2825 - accuracy: 0.9142 - val_loss: 1.0059 - val_accuracy: 0.7260\nEpoch 112/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.2915 - accuracy: 0.9079 - val_loss: 1.0537 - val_accuracy: 0.7123\nEpoch 113/1000\n5/5 [==============================] - 0s 48ms/step - loss: 0.2765 - accuracy: 0.9152 - val_loss: 1.0107 - val_accuracy: 0.7123\nEpoch 114/1000\n5/5 [==============================] - 0s 54ms/step - loss: 0.2673 - accuracy: 0.9268 - val_loss: 1.0938 - val_accuracy: 0.6795\nEpoch 115/1000\n5/5 [==============================] - 0s 50ms/step - loss: 0.4083 - accuracy: 0.8682 - val_loss: 1.0938 - val_accuracy: 0.7096\nEpoch 116/1000\n5/5 [==============================] - 0s 41ms/step - loss: 0.4274 - accuracy: 0.8580 - val_loss: 1.0664 - val_accuracy: 0.7178\nEpoch 117/1000\n5/5 [==============================] - 0s 41ms/step - loss: 0.3461 - accuracy: 0.8837 - val_loss: 1.0762 - val_accuracy: 0.7123\nEpoch 118/1000\n5/5 [==============================] - 0s 40ms/step - loss: 0.2928 - accuracy: 0.9118 - val_loss: 1.0645 - val_accuracy: 0.7178\nEpoch 119/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.3005 - accuracy: 0.9016 - val_loss: 1.0518 - val_accuracy: 0.7123\nEpoch 120/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2773 - accuracy: 0.9167 - val_loss: 1.0283 - val_accuracy: 0.7178\nEpoch 121/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2535 - accuracy: 0.9147 - val_loss: 1.0527 - val_accuracy: 0.7260\nEpoch 122/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.2395 - accuracy: 0.9302 - val_loss: 1.0684 - val_accuracy: 0.7260\nEpoch 123/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2475 - accuracy: 0.9254 - val_loss: 1.0449 - val_accuracy: 0.7178\nEpoch 124/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.2252 - accuracy: 0.9360 - val_loss: 1.0332 - val_accuracy: 0.7178\nEpoch 125/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2212 - accuracy: 0.9360 - val_loss: 1.0156 - val_accuracy: 0.7123\nEpoch 126/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2220 - accuracy: 0.9404 - val_loss: 1.0156 - val_accuracy: 0.7068\nEpoch 127/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.2102 - accuracy: 0.9457 - val_loss: 1.0283 - val_accuracy: 0.7233\nEpoch 128/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.2130 - accuracy: 0.9419 - val_loss: 1.0322 - val_accuracy: 0.7205\nEpoch 129/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2103 - accuracy: 0.9409 - val_loss: 1.0293 - val_accuracy: 0.7233\nEpoch 130/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.2114 - accuracy: 0.9438 - val_loss: 1.0225 - val_accuracy: 0.7233\nEpoch 131/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2064 - accuracy: 0.9390 - val_loss: 1.0332 - val_accuracy: 0.7260\nEpoch 132/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1962 - accuracy: 0.9433 - val_loss: 1.0557 - val_accuracy: 0.7178\nEpoch 133/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2080 - accuracy: 0.9370 - val_loss: 1.0576 - val_accuracy: 0.7205\nEpoch 134/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1887 - accuracy: 0.9525 - val_loss: 1.0430 - val_accuracy: 0.7178\nEpoch 135/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.2077 - accuracy: 0.9399 - val_loss: 1.0498 - val_accuracy: 0.7205\nEpoch 136/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1997 - accuracy: 0.9433 - val_loss: 1.0518 - val_accuracy: 0.7288\nEpoch 137/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1823 - accuracy: 0.9501 - val_loss: 1.0566 - val_accuracy: 0.7205\nEpoch 138/1000\n5/5 [==============================] - 0s 48ms/step - loss: 0.1895 - accuracy: 0.9486 - val_loss: 1.0479 - val_accuracy: 0.7205\nEpoch 139/1000\n5/5 [==============================] - 0s 39ms/step - loss: 0.1830 - accuracy: 0.9491 - val_loss: 1.0449 - val_accuracy: 0.7205\nEpoch 140/1000\n5/5 [==============================] - 0s 42ms/step - loss: 0.1844 - accuracy: 0.9457 - val_loss: 1.0430 - val_accuracy: 0.7260\nEpoch 141/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1809 - accuracy: 0.9525 - val_loss: 1.0742 - val_accuracy: 0.7151\nEpoch 142/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1834 - accuracy: 0.9506 - val_loss: 1.0762 - val_accuracy: 0.7151\nEpoch 143/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.1758 - accuracy: 0.9520 - val_loss: 1.0752 - val_accuracy: 0.7151\nEpoch 144/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1707 - accuracy: 0.9564 - val_loss: 1.0742 - val_accuracy: 0.7151\nEpoch 145/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1764 - accuracy: 0.9525 - val_loss: 1.0752 - val_accuracy: 0.7178\nEpoch 146/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1734 - accuracy: 0.9554 - val_loss: 1.0752 - val_accuracy: 0.7233\nEpoch 147/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1747 - accuracy: 0.9525 - val_loss: 1.0742 - val_accuracy: 0.7260\nEpoch 148/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1649 - accuracy: 0.9578 - val_loss: 1.0703 - val_accuracy: 0.7260\nEpoch 149/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1644 - accuracy: 0.9564 - val_loss: 1.0674 - val_accuracy: 0.7260\nEpoch 150/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1656 - accuracy: 0.9564 - val_loss: 1.0654 - val_accuracy: 0.7315\nEpoch 151/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1662 - accuracy: 0.9496 - val_loss: 1.0635 - val_accuracy: 0.7288\nEpoch 152/1000\n5/5 [==============================] - 0s 39ms/step - loss: 0.1610 - accuracy: 0.9545 - val_loss: 1.0596 - val_accuracy: 0.7260\nEpoch 153/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1613 - accuracy: 0.9549 - val_loss: 1.0596 - val_accuracy: 0.7260\nEpoch 154/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1648 - accuracy: 0.9574 - val_loss: 1.0596 - val_accuracy: 0.7260\nEpoch 155/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1620 - accuracy: 0.9545 - val_loss: 1.0645 - val_accuracy: 0.7288\nEpoch 156/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1672 - accuracy: 0.9535 - val_loss: 1.0605 - val_accuracy: 0.7288\nEpoch 157/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1624 - accuracy: 0.9549 - val_loss: 1.0566 - val_accuracy: 0.7288\nEpoch 158/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1570 - accuracy: 0.9588 - val_loss: 1.0615 - val_accuracy: 0.7260\nEpoch 159/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1574 - accuracy: 0.9583 - val_loss: 1.0693 - val_accuracy: 0.7205\nEpoch 160/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1538 - accuracy: 0.9593 - val_loss: 1.0791 - val_accuracy: 0.7178\nEpoch 161/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1644 - accuracy: 0.9588 - val_loss: 1.0928 - val_accuracy: 0.7205\nEpoch 162/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1535 - accuracy: 0.9569 - val_loss: 1.0947 - val_accuracy: 0.7315\nEpoch 163/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1506 - accuracy: 0.9608 - val_loss: 1.0918 - val_accuracy: 0.7315\nEpoch 164/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1555 - accuracy: 0.9588 - val_loss: 1.0898 - val_accuracy: 0.7342\nEpoch 165/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1546 - accuracy: 0.9637 - val_loss: 1.0889 - val_accuracy: 0.7288\nEpoch 166/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1554 - accuracy: 0.9608 - val_loss: 1.0879 - val_accuracy: 0.7233\nEpoch 167/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1483 - accuracy: 0.9608 - val_loss: 1.0879 - val_accuracy: 0.7205\nEpoch 168/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1527 - accuracy: 0.9622 - val_loss: 1.0889 - val_accuracy: 0.7178\nEpoch 169/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1515 - accuracy: 0.9564 - val_loss: 1.0898 - val_accuracy: 0.7205\nEpoch 170/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1482 - accuracy: 0.9617 - val_loss: 1.0898 - val_accuracy: 0.7205\nEpoch 171/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.1543 - accuracy: 0.9598 - val_loss: 1.0908 - val_accuracy: 0.7205\nEpoch 172/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.1527 - accuracy: 0.9588 - val_loss: 1.0918 - val_accuracy: 0.7233\nEpoch 173/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1543 - accuracy: 0.9574 - val_loss: 1.0957 - val_accuracy: 0.7260\nEpoch 174/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1488 - accuracy: 0.9603 - val_loss: 1.0977 - val_accuracy: 0.7288\nEpoch 175/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1454 - accuracy: 0.9646 - val_loss: 1.0977 - val_accuracy: 0.7288\nEpoch 176/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1487 - accuracy: 0.9598 - val_loss: 1.0986 - val_accuracy: 0.7288\nEpoch 177/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1455 - accuracy: 0.9583 - val_loss: 1.0967 - val_accuracy: 0.7260\nEpoch 178/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1471 - accuracy: 0.9627 - val_loss: 1.0957 - val_accuracy: 0.7260\nEpoch 179/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1485 - accuracy: 0.9574 - val_loss: 1.0967 - val_accuracy: 0.7260\nEpoch 180/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1435 - accuracy: 0.9646 - val_loss: 1.0957 - val_accuracy: 0.7260\nEpoch 181/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1468 - accuracy: 0.9622 - val_loss: 1.0938 - val_accuracy: 0.7205\nEpoch 182/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1479 - accuracy: 0.9598 - val_loss: 1.0938 - val_accuracy: 0.7178\nEpoch 183/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.1472 - accuracy: 0.9617 - val_loss: 1.0957 - val_accuracy: 0.7205\nEpoch 184/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1468 - accuracy: 0.9593 - val_loss: 1.0967 - val_accuracy: 0.7205\nEpoch 185/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1521 - accuracy: 0.9588 - val_loss: 1.0977 - val_accuracy: 0.7205\nEpoch 186/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1536 - accuracy: 0.9588 - val_loss: 1.0977 - val_accuracy: 0.7205\nEpoch 187/1000\n5/5 [==============================] - 0s 39ms/step - loss: 0.1404 - accuracy: 0.9637 - val_loss: 1.0967 - val_accuracy: 0.7205\nEpoch 188/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.1429 - accuracy: 0.9637 - val_loss: 1.0967 - val_accuracy: 0.7205\nEpoch 189/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1460 - accuracy: 0.9646 - val_loss: 1.0967 - val_accuracy: 0.7233\nEpoch 190/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1430 - accuracy: 0.9593 - val_loss: 1.0967 - val_accuracy: 0.7178\nEpoch 191/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1464 - accuracy: 0.9608 - val_loss: 1.0947 - val_accuracy: 0.7178\nEpoch 192/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1410 - accuracy: 0.9632 - val_loss: 1.0947 - val_accuracy: 0.7205\nEpoch 193/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1464 - accuracy: 0.9617 - val_loss: 1.0957 - val_accuracy: 0.7178\nEpoch 194/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1380 - accuracy: 0.9612 - val_loss: 1.0996 - val_accuracy: 0.7178\nEpoch 195/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1429 - accuracy: 0.9646 - val_loss: 1.1035 - val_accuracy: 0.7178\nEpoch 196/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1480 - accuracy: 0.9603 - val_loss: 1.1045 - val_accuracy: 0.7178\nEpoch 197/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1396 - accuracy: 0.9627 - val_loss: 1.1055 - val_accuracy: 0.7178\nEpoch 198/1000\n5/5 [==============================] - 0s 49ms/step - loss: 0.1398 - accuracy: 0.9598 - val_loss: 1.1074 - val_accuracy: 0.7178\nEpoch 199/1000\n5/5 [==============================] - 0s 43ms/step - loss: 0.1412 - accuracy: 0.9637 - val_loss: 1.1084 - val_accuracy: 0.7178\nEpoch 200/1000\n5/5 [==============================] - 0s 41ms/step - loss: 0.1472 - accuracy: 0.9578 - val_loss: 1.1094 - val_accuracy: 0.7151\nEpoch 201/1000\n5/5 [==============================] - 0s 41ms/step - loss: 0.1393 - accuracy: 0.9632 - val_loss: 1.1094 - val_accuracy: 0.7151\nEpoch 202/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1404 - accuracy: 0.9622 - val_loss: 1.1094 - val_accuracy: 0.7178\nEpoch 203/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1443 - accuracy: 0.9612 - val_loss: 1.1104 - val_accuracy: 0.7233\nEpoch 204/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1410 - accuracy: 0.9608 - val_loss: 1.1113 - val_accuracy: 0.7205\nEpoch 205/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1400 - accuracy: 0.9627 - val_loss: 1.1113 - val_accuracy: 0.7205\nEpoch 206/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1360 - accuracy: 0.9627 - val_loss: 1.1123 - val_accuracy: 0.7205\nEpoch 207/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1388 - accuracy: 0.9637 - val_loss: 1.1133 - val_accuracy: 0.7205\nEpoch 208/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1345 - accuracy: 0.9641 - val_loss: 1.1133 - val_accuracy: 0.7205\nEpoch 209/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1412 - accuracy: 0.9646 - val_loss: 1.1133 - val_accuracy: 0.7205\nEpoch 210/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1370 - accuracy: 0.9675 - val_loss: 1.1143 - val_accuracy: 0.7205\nEpoch 211/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1432 - accuracy: 0.9632 - val_loss: 1.1152 - val_accuracy: 0.7233\nEpoch 212/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1375 - accuracy: 0.9627 - val_loss: 1.1162 - val_accuracy: 0.7233\nEpoch 213/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1394 - accuracy: 0.9666 - val_loss: 1.1172 - val_accuracy: 0.7233\nEpoch 214/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1320 - accuracy: 0.9671 - val_loss: 1.1162 - val_accuracy: 0.7233\nEpoch 215/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1389 - accuracy: 0.9617 - val_loss: 1.1172 - val_accuracy: 0.7205\nEpoch 216/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1317 - accuracy: 0.9675 - val_loss: 1.1182 - val_accuracy: 0.7178\nEpoch 217/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1359 - accuracy: 0.9641 - val_loss: 1.1182 - val_accuracy: 0.7178\nEpoch 218/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1396 - accuracy: 0.9641 - val_loss: 1.1152 - val_accuracy: 0.7178\nEpoch 219/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1355 - accuracy: 0.9661 - val_loss: 1.1143 - val_accuracy: 0.7178\nEpoch 220/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1347 - accuracy: 0.9656 - val_loss: 1.1133 - val_accuracy: 0.7178\nEpoch 221/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1334 - accuracy: 0.9666 - val_loss: 1.1133 - val_accuracy: 0.7205\nEpoch 222/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1376 - accuracy: 0.9661 - val_loss: 1.1113 - val_accuracy: 0.7205\nEpoch 223/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1274 - accuracy: 0.9671 - val_loss: 1.1094 - val_accuracy: 0.7178\nEpoch 224/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1364 - accuracy: 0.9646 - val_loss: 1.1104 - val_accuracy: 0.7178\nEpoch 225/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1354 - accuracy: 0.9637 - val_loss: 1.1113 - val_accuracy: 0.7178\nEpoch 226/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1369 - accuracy: 0.9617 - val_loss: 1.1113 - val_accuracy: 0.7151\nEpoch 227/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1347 - accuracy: 0.9646 - val_loss: 1.1123 - val_accuracy: 0.7151\nEpoch 228/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1371 - accuracy: 0.9617 - val_loss: 1.1123 - val_accuracy: 0.7123\nEpoch 229/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.1332 - accuracy: 0.9656 - val_loss: 1.1133 - val_accuracy: 0.7123\nEpoch 230/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1354 - accuracy: 0.9627 - val_loss: 1.1152 - val_accuracy: 0.7151\nEpoch 231/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1328 - accuracy: 0.9632 - val_loss: 1.1162 - val_accuracy: 0.7151\nEpoch 232/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1391 - accuracy: 0.9632 - val_loss: 1.1162 - val_accuracy: 0.7178\nEpoch 233/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1324 - accuracy: 0.9685 - val_loss: 1.1172 - val_accuracy: 0.7178\nEpoch 234/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1377 - accuracy: 0.9632 - val_loss: 1.1162 - val_accuracy: 0.7123\nEpoch 235/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1354 - accuracy: 0.9641 - val_loss: 1.1162 - val_accuracy: 0.7096\nEpoch 236/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1353 - accuracy: 0.9651 - val_loss: 1.1162 - val_accuracy: 0.7068\nEpoch 237/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1354 - accuracy: 0.9656 - val_loss: 1.1152 - val_accuracy: 0.7068\nEpoch 238/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1333 - accuracy: 0.9666 - val_loss: 1.1152 - val_accuracy: 0.7123\nEpoch 239/1000\n5/5 [==============================] - 0s 37ms/step - loss: 0.1328 - accuracy: 0.9641 - val_loss: 1.1152 - val_accuracy: 0.7178\nEpoch 240/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1280 - accuracy: 0.9646 - val_loss: 1.1182 - val_accuracy: 0.7151\nEpoch 241/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1328 - accuracy: 0.9695 - val_loss: 1.1211 - val_accuracy: 0.7151\nEpoch 242/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1309 - accuracy: 0.9637 - val_loss: 1.1221 - val_accuracy: 0.7151\nEpoch 243/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1333 - accuracy: 0.9627 - val_loss: 1.1221 - val_accuracy: 0.7178\nEpoch 244/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1294 - accuracy: 0.9656 - val_loss: 1.1230 - val_accuracy: 0.7178\nEpoch 245/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1324 - accuracy: 0.9646 - val_loss: 1.1260 - val_accuracy: 0.7233\nEpoch 246/1000\n5/5 [==============================] - 0s 38ms/step - loss: 0.1251 - accuracy: 0.9685 - val_loss: 1.1270 - val_accuracy: 0.7233\nEpoch 247/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1362 - accuracy: 0.9627 - val_loss: 1.1289 - val_accuracy: 0.7233\nEpoch 248/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1316 - accuracy: 0.9627 - val_loss: 1.1299 - val_accuracy: 0.7233\nEpoch 249/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1287 - accuracy: 0.9690 - val_loss: 1.1309 - val_accuracy: 0.7233\nEpoch 250/1000\n5/5 [==============================] - 0s 36ms/step - loss: 0.1307 - accuracy: 0.9680 - val_loss: 1.1318 - val_accuracy: 0.7205\nEpoch 251/1000\n5/5 [==============================] - 0s 35ms/step - loss: 0.1269 - accuracy: 0.9690 - val_loss: 1.1309 - val_accuracy: 0.7205\nEpoch 252/1000\n5/5 [==============================] - 0s 39ms/step - loss: 0.1398 - accuracy: 0.9612 - val_loss: 1.1309 - val_accuracy: 0.7205\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Convolutional model evaluation","metadata":{}},{"cell_type":"code","source":"if not use_cross_valid:\n    plot_history(history)\n\n    # Predict the test set with the LSTM\n    predictions = model.predict(X_test)\n\n    plot_statistics(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T22:12:02.369712Z","iopub.execute_input":"2022-12-18T22:12:02.370415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save model with the validation accuracy in the name","metadata":{}},{"cell_type":"code","source":"if not use_cross_valid:\n    accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n    if (accuracy > 0.765):\n        model_name = \"TUNED_GRU_\" + str(accuracy.round(6))\n        model.save(model_name)\n\n        \n        FileLink(r\"\" + model_name + \".zip\")\n        import joblib\n        scaler_std_filename =model_name+ \"/scaler_std.save\"\n        joblib.dump(scaler_std, scaler_std_filename) \n        scaler_0_filename =model_name+ \"/scaler_0.save\"\n        joblib.dump(scaler_0, scaler_0_filename) \n        #scaler_robust_filename =model_name+ \"/scaler_robust.save\"\n        #joblib.dump(scaler_robust, scaler_robust_filename) \n\n        shutil.make_archive(model_name, 'zip', model_name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cross Validation","metadata":{}},{"cell_type":"code","source":"def scale_feature_wise(X_train, X_test):  #apply rescaling to each feature\n    scaler_robust = RobustScaler()\n    scaler_std = StandardScaler()\n    scaler_0 = MinMaxScaler(feature_range=(0,1))\n    scaler_1 = MinMaxScaler(feature_range=(-1,1))\n\n    num_instances, num_time_steps, num_features = X_train.shape\n    X_train = np.reshape(X_train, newshape=(-1, num_features))\n    #X_train = scaler_robust.fit_transform(X_train)\n    X_train = scaler_0.fit_transform(X_train)\n    X_train = scaler_std.fit_transform(X_train)\n\n    X_train = np.reshape(X_train, newshape=(num_instances, num_time_steps, num_features))\n\n        \n    \n    if X_test is not None:\n        num_instances, num_time_steps, num_features = X_test.shape\n        X_test = np.reshape(X_test, newshape=(-1, num_features))\n        #X_test = scaler_robust.transform(X_test)\n        X_test = scaler_0.transform(X_test)\n        X_test = scaler_std.transform(X_test)\n\n        X_test = np.reshape(X_test, newshape=(num_instances, num_time_steps, num_features))\n        return X_train, X_test\n    \n    return X_train\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale_feature_wise_pca(X_train, X_test, n_components = 6):  #apply rescaling to each feature and use pca to reduce the number of features\n    from sklearn.decomposition import PCA\n    \n    scaler_robust = RobustScaler()\n    scaler_std = StandardScaler()\n    scaler_0 = MinMaxScaler(feature_range=(0,1))\n    scaler_1 = MinMaxScaler(feature_range=(-1,1))\n\n    num_instances, num_time_steps, num_features = X_train.shape\n    X_train = np.reshape(X_train, newshape=(-1, num_features))\n    #X_train = scaler_robust.fit_transform(X_train)\n    X_train = scaler_0.fit_transform(X_train)\n    X_train = scaler_std.fit_transform(X_train)\n    \n    pca = PCA(n_components=n_components)\n    pca.fit(X_train)\n    X_train = pca.transform(X_train)\n    num_features = pca.n_components_\n    print(\"(\"+str(num_features)+\" PCA components)\")\n    \n    X_train = np.reshape(X_train, newshape=(num_instances, num_time_steps, num_features))\n\n        \n    \n    if X_test is not None:\n        num_instances, num_time_steps, num_features = X_test.shape\n        X_test = np.reshape(X_test, newshape=(-1, num_features))\n        #X_test = scaler_robust.transform(X_test)\n        X_test = scaler_0.transform(X_test)\n        X_test = scaler_std.transform(X_test)\n        \n        X_test = pca.transform(X_test)\n        num_features = pca.n_components_\n        \n        X_test = np.reshape(X_test, newshape=(num_instances, num_time_steps, num_features))\n        return X_train, X_test\n    \n    return X_train\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale_ts_feature_wise(X_train, X_test):  #apply rescaling to each (timestamp,feature) pair\n    scaler_robust = RobustScaler()\n    scaler_std = StandardScaler()\n    scaler_0 = MinMaxScaler(feature_range=(0,1))\n    scaler_1 = MinMaxScaler(feature_range=(-1,1))\n\n    num_instances, num_time_steps, num_features = X_train.shape\n    X_train = np.reshape(X_train, newshape=(-1, num_features * num_time_steps))\n    #X_train = scaler_robust.fit_transform(X_train)\n    X_train = scaler_0.fit_transform(X_train)\n    X_train = scaler_std.fit_transform(X_train)\n\n    X_train = np.reshape(X_train, newshape=(num_instances, num_time_steps, num_features))\n\n        \n    \n    if X_test is not None:\n        num_instances, num_time_steps, num_features = X_test.shape\n        X_test = np.reshape(X_test, newshape=(-1, num_features * num_time_steps))\n        #X_test = scaler_robust.transform(X_test)\n        X_test = scaler_0.transform(X_test)\n        X_test = scaler_std.transform(X_test)\n\n        X_test = np.reshape(X_test, newshape=(num_instances, num_time_steps, num_features))\n        return X_train, X_test\n    \n    return X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import class_weight\n# Define the K-fold Cross Validator\nnum_folds = 10\nkfold = StratifiedKFold(n_splits=num_folds, shuffle=True) #Shuffle as we consider each time-series independent from others","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing timeseries augmentation with crossvalidation\n\nif use_cross_valid:\n    best_acc = 0\n    best_model = None\n    best_history =  []\n    \n    # grid search FORs go here\n    #print(f' MODEL {i} ')\n    \n    print(f' START ')\n\n    acc_per_fold = []\n    loss_per_fold = []\n    fold_no = 1\n    for train, test in kfold.split(X, y):\n        print(f'Training for fold {fold_no} ...', end=' ')\n        \n        X_train = np.copy(X[train])\n        y_train = np.copy(y[train])\n        X_test = np.copy(X[test])\n        y_test = np.copy(y[test])\n        \n        #drop some feature\n        #X_train = np.delete(X_train, 5, axis=2)\n        #X_test = np.delete(X_test, 5, axis=2)\n        \n        #test splines augmentation: creating new synthetic samples\n        #by increasing the resolution of original series and adding gaussian noise\n        #note that training will be much slower since augmentation is computed by CPU\n        #and interpolation_multiplier linearly increases training set size\n        \n        #X_train,y_train = spline_augment_no_print(X_train,y_train,interpolation_multiplier = 3,augment_std = 0.05)\n        \n        y_train = tfk.utils.to_categorical(y_train)\n        y_test = tfk.utils.to_categorical(y_test)\n        classes = y_train.shape[-1]\n        \n        \n        # Apply scaling (use only one of the following two)\n        #X_train,X_test = scale_ts_feature_wise(X_train, X_test)\n        X_train,X_test = scale_feature_wise(X_train, X_test)\n        #X_train,X_test = scale_feature_wise_pca(X_train, X_test, n_components = 6)\n        \n        input_shape = X_train.shape[1:]\n         # Intantiate a new model\n            \n        model = build_classifier(input_shape, classes)\n        \n        history = model.fit(X_train, y_train,\n                  batch_size = batch_size,\n                  epochs = epochs,\n                  validation_data = (X_test, y_test),\n                  #class_weight = class_weights,\n                  verbose=0,\n                  callbacks = [\n                      tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=100, restore_best_weights=True),\n                      tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=20, factor=0.4, min_lr=5e-5)\n                  ]\n        ).history\n        \n        # Generate generalization metrics\n        acc = np.max(history['val_accuracy']) * 100\n        print(f'score: accuracy of {acc}%')\n        acc_per_fold.append(acc)\n\n        # Increase fold number\n        fold_no = fold_no + 1\n        if fold_no <= num_folds + 1:\n            print('------------------------------------------------------------------------')\n\n    print('\\nAverage scores for all folds:')\n    acc = np.mean(acc_per_fold)\n    print(f'> Accuracy: {acc} (+- {np.std(acc_per_fold)})')\n    print(f'> Loss: {np.mean(loss_per_fold)}')\n    if acc > best_acc:\n        best_acc = acc\n        best_model = model\n        best_history = history\n    \n    \n    print('')\n              \n    # Plot results\n    best_model.summary()\n    plot_history(best_history)\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}