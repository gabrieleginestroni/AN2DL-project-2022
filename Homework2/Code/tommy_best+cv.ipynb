{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### General imports and definitions","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport shutil\nimport random\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.utils import class_weight\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rc('font', size=16) \n\ntfk = tf.keras\ntfkl = tf.keras.layers\nprint(tf.__version__)\nprint(tf.config.list_physical_devices())","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.380949Z","iopub.execute_input":"2022-12-16T16:19:33.381410Z","iopub.status.idle":"2022-12-16T16:19:33.391474Z","shell.execute_reply.started":"2022-12-16T16:19:33.381373Z","shell.execute_reply":"2022-12-16T16:19:33.390189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suppress warnings","metadata":{}},{"cell_type":"code","source":"import warnings\nimport logging\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\ntf.get_logger().setLevel('INFO')\ntf.autograph.set_verbosity(0)\n\ntf.get_logger().setLevel(logging.ERROR)\ntf.get_logger().setLevel('ERROR')\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.393176Z","iopub.execute_input":"2022-12-16T16:19:33.393517Z","iopub.status.idle":"2022-12-16T16:19:33.407746Z","shell.execute_reply.started":"2022-12-16T16:19:33.393486Z","shell.execute_reply":"2022-12-16T16:19:33.406375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set seed for reproducibility","metadata":{}},{"cell_type":"code","source":"# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.409812Z","iopub.execute_input":"2022-12-16T16:19:33.410787Z","iopub.status.idle":"2022-12-16T16:19:33.421395Z","shell.execute_reply.started":"2022-12-16T16:19:33.410745Z","shell.execute_reply":"2022-12-16T16:19:33.419925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load and clean the dataset","metadata":{}},{"cell_type":"code","source":"# Load the .npy file\nX = np.load(r'../input/training/x_train.npy')\ny = np.load(r'../input/training/y_train.npy').astype(np.int8)\n\n# Note as sequences are already built, with window 36\nprint(X.shape, X.dtype, sep=\", \")   \nprint(y.shape, y.dtype, sep=\", \") \n\nprint(X[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.426035Z","iopub.execute_input":"2022-12-16T16:19:33.426476Z","iopub.status.idle":"2022-12-16T16:19:33.450010Z","shell.execute_reply.started":"2022-12-16T16:19:33.426441Z","shell.execute_reply":"2022-12-16T16:19:33.448315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop features","metadata":{}},{"cell_type":"code","source":"def keep_only(X, features_list):\n    n_instances, n_timestamps, n_features = X.shape\n    X = np.reshape(X, newshape=(-1, n_features))\n    temp = np.empty((n_instances * n_timestamps, len(features_list)))\n    \n    i = 0\n    for f in features_list:\n        temp[:, i] = X[:, f]\n        i += 1\n    \n    temp = np.reshape(temp, newshape=(n_instances, n_timestamps, len(features_list)))\n    return temp\n\ndrop = False\n\nif drop:\n    X = keep_only(X, [1, 2, 3, 4, 5])\n    \nprint(X.shape)\nprint(X[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.452313Z","iopub.execute_input":"2022-12-16T16:19:33.452709Z","iopub.status.idle":"2022-12-16T16:19:33.466485Z","shell.execute_reply.started":"2022-12-16T16:19:33.452674Z","shell.execute_reply":"2022-12-16T16:19:33.464850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the class weights in order to balance loss during training\nlabels = np.unique(np.fromiter([t for t in y], np.int32))\nclass_weights = dict(enumerate(class_weight.compute_class_weight('balanced', classes=labels, y=y)))\nprint(class_weights, '\\n')\n\n# Convert the sparse labels to categorical values\ny = tfk.utils.to_categorical(y)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.467946Z","iopub.execute_input":"2022-12-16T16:19:33.468471Z","iopub.status.idle":"2022-12-16T16:19:33.523287Z","shell.execute_reply.started":"2022-12-16T16:19:33.468419Z","shell.execute_reply":"2022-12-16T16:19:33.522054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Metadata","metadata":{}},{"cell_type":"code","source":"input_shape = X_train.shape[1:]\nn_timestamps, n_features = input_shape\nclasses = y_train.shape[-1]\nlabel_mapping = {\n    'Wish': 0,\n    'Another': 1,\n    'Comfortably': 2,\n    'Money': 3,\n    'Breathe': 4,\n    'Time': 5,\n    'Brain': 6,\n    'Echoes': 7,\n    'Wearing': 8,\n    'Sorrow': 9,\n    'Hey': 10,\n    'Shine': 11,  \n}\n\n# For training\nbatch_size = 512\nepochs = 1000\n\n# Define the scaler we intend to use\nscaler = StandardScaler()\n\n# Define the K-fold Cross Validator\nnum_folds = 10\nkfold = KFold(n_splits=num_folds, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.524695Z","iopub.execute_input":"2022-12-16T16:19:33.525082Z","iopub.status.idle":"2022-12-16T16:19:33.534380Z","shell.execute_reply.started":"2022-12-16T16:19:33.525046Z","shell.execute_reply":"2022-12-16T16:19:33.532788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data scaling","metadata":{}},{"cell_type":"code","source":"def scale(X_train, X_test, scaler):\n    n_instances_train = X_train.shape[0]\n    X_train = np.reshape(X_train, newshape=(-1, n_features))\n    X_train = scaler.fit_transform(X_train)\n    X_train = np.reshape(X_train, newshape=(n_instances_train, n_timestamps, n_features))\n    \n    if X_test is not None:\n        n_instances_test = X_test.shape[0]\n        X_test = np.reshape(X_test, newshape=(-1, n_features))\n        X_test = scaler.transform(X_test)\n        X_test = np.reshape(X_test, newshape=(n_instances_test, n_timestamps, n_features))\n\nscaling = True\n\nif scaling:\n    scale(X_train, X_test, scaler)","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.537784Z","iopub.execute_input":"2022-12-16T16:19:33.538372Z","iopub.status.idle":"2022-12-16T16:19:33.569172Z","shell.execute_reply.started":"2022-12-16T16:19:33.538319Z","shell.execute_reply":"2022-12-16T16:19:33.567363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Support functions","metadata":{}},{"cell_type":"code","source":"def plot_history(history):\n    best_epoch = np.argmax(history['val_accuracy'])\n    plt.figure(figsize=(17,4))\n    plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n    plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n    plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n    plt.title('Categorical Crossentropy')\n    plt.legend()\n    plt.grid(alpha=.3)\n    plt.show()\n\n    plt.figure(figsize=(17,4))\n    plt.plot(history['accuracy'], label='Training accuracy', alpha=.8, color='#ff7f0e')\n    plt.plot(history['val_accuracy'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\n    plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n    plt.title('Accuracy')\n    plt.legend()\n    plt.grid(alpha=.3)\n    plt.show()\n\n    plt.figure(figsize=(17,4))\n    plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n    plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n    plt.legend()\n    plt.grid(alpha=.3)\n    plt.show()\n\ndef plot_cm(model):\n    # Predict the test set with the LSTM\n    predictions = model.predict(X_test)\n    predictions.shape\n\n    # Compute the confusion matrix\n    cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n\n    # Compute the classification metrics\n    accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n    precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n    recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n    f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average=None)\n    print('Accuracy:',accuracy.round(4))\n    print('Precision:',precision.round(4))\n    print('Recall:',recall.round(4))\n    print('F1:',f1.round(4))\n\n    # Plot the confusion matrix\n    plt.figure(figsize=(10,8))\n    sns.heatmap(cm.T, cmap='Blues', xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()), annot=True, fmt=\"d\")\n    plt.xlabel('True labels')\n    plt.ylabel('Predicted labels')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.571055Z","iopub.execute_input":"2022-12-16T16:19:33.571553Z","iopub.status.idle":"2022-12-16T16:19:33.592210Z","shell.execute_reply.started":"2022-12-16T16:19:33.571503Z","shell.execute_reply":"2022-12-16T16:19:33.590063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Classifier","metadata":{}},{"cell_type":"code","source":"# we advice to choose what parameters to change and include them directly among the attributes of this function\ndef build_TS_classifier(input_shape, classes):\n    \n    input_layer = tfkl.Input(shape=input_shape)\n    \n    x = tfkl.Conv1D(\n        filters = 256,\n        kernel_size = 3,\n        padding = 'same',\n        activation = 'relu'\n        )(input_layer)\n    \n    x = tfkl.Conv1D(\n        filters = 256,\n        kernel_size = 3,\n        padding = 'same',\n        activation = 'relu'\n        )(x)\n    \n    #x = tfkl.MaxPooling1D()(x)\n\n    x = tfkl.Conv1D(\n        filters = 512,\n        kernel_size = 3,\n        padding = 'same',\n        activation = 'relu'\n        )(x)\n    \n    #x = tfkl.MaxPooling1D()(x)\n\n    x = tfkl.Conv1D(\n        filters = 256,\n        kernel_size = 3,\n        padding = 'same',\n        activation = 'relu'\n        )(x)\n    \n    x = tfkl.Conv1D(\n        filters = 256,\n        kernel_size = 3,\n        padding = 'same',\n        activation = 'relu'\n        )(x)\n    \n    x = tfkl.GlobalAveragePooling1D()(x)\n    \n    # Classifier\n    x = tfkl.Dense(\n        units = 256,  \n        activation = 'relu'\n        )(x)\n    \n    x = tfkl.Dropout(0.2, seed=seed)(x)\n\n    x = tfkl.Dense(\n        units = 128,  \n        activation = 'relu'\n        )(x)\n    \n    x = tfkl.Dropout(0.2, seed=seed)(x)\n    \n    x = tfkl.Dense(\n        units = 128,  \n        activation = 'relu'\n        )(x)\n    \n    x = tfkl.Dropout(0.2, seed=seed)(x)\n\n    output_layer = tfkl.Dense(classes, activation='softmax')(x)\n\n    # Connect input and output through the Model class\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='cnn_model')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-3), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.593591Z","iopub.execute_input":"2022-12-16T16:19:33.594440Z","iopub.status.idle":"2022-12-16T16:19:33.609488Z","shell.execute_reply.started":"2022-12-16T16:19:33.594400Z","shell.execute_reply":"2022-12-16T16:19:33.608541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_val = True\nfinal_train = True\n\nif cross_val:\n    best_acc = 0\n    best_model = None\n    best_history =  []\n    \n    # grid search FORs go here (remember to indent the following code: SEE THEN)\n    #print(f'°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°° MODEL {i} °°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°')\n    \n    print(f'°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°° START °°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°')\n\n    acc_per_fold = []\n    fold_no = 1\n    for train, test in kfold.split(X, y):\n        print(f'Training for fold {fold_no} ...', end=' ')\n\n        # Intantiate a new model\n        model = build_TS_classifier(input_shape, classes)\n        \n        # Apply scaling\n        X_train = np.copy(X[train])\n        y_train = np.copy(y[train])\n        X_test = np.copy(X[test])\n        y_test = np.copy(y[test])\n        scale(X_train, X_test, scaler)\n\n        history = model.fit(X_train, y_train,\n                  batch_size = batch_size,\n                  epochs = epochs,\n                  validation_data = (X_test, y_test),\n                  #class_weight = class_weights,\n                  verbose = 0,\n                  callbacks = [\n                      tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=51, restore_best_weights=True),\n                      tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=5, factor=tf.math.exp(-0.1), min_lr=2e-5)\n                  ]\n        ).history\n\n        # Generate generalization metrics\n        acc = np.max(history['val_accuracy']) * 100\n        print(f'score: accuracy of {acc:.3f}%')\n        acc_per_fold.append(acc)\n\n        # Increase fold number\n        fold_no = fold_no + 1\n        if fold_no <= num_folds + 1:\n            print('------------------------------------------------------------------------')\n\n    print('\\nAverage scores for all folds:')\n    acc = np.mean(acc_per_fold)\n    print(f'> Accuracy: {acc} (+- {np.std(acc_per_fold)})')\n    if acc > best_acc:\n        best_acc = acc\n        best_model = model  # the last model is taken\n        best_history = history\n    \n    # INDENT TILL THE PREVIOUS LINE\n    print('°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°')\n              \n    # Plot results\n    best_model.summary()\n    plot_history(best_history)\n\n    # RETRAIN THE BEST MODEL ON THE WHOLE DATASET\n    if final_train:\n        \n        # Reinstanciate the model (if we have parameters we must pass them over)\n        best_model = build_TS_classifier(input_shape, classes)\n        \n        # Apply scaling\n        scale(X, None, scaler)\n        \n        best_model.fit(\n            x = X,\n            y = y,\n            batch_size = batch_size,\n            epochs = epochs,\n            #class_weight = class_weights,\n            callbacks = [\n                tfk.callbacks.EarlyStopping(monitor='accuracy', mode='max', patience=81, restore_best_weights=True),\n                tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=10, factor=tf.math.exp(-0.1), min_lr=2e-5)\n            ]\n        )\n    \nelse:\n    best_model = build_TS_classifier(input_shape, classes)\n    best_model.summary()\n    \n    # Train the model\n    best_history = best_model.fit(\n        x = X_train,\n        y = y_train,\n        batch_size = batch_size,\n        epochs = epochs,\n        validation_data = (X_test, y_test),\n        #class_weight = class_weights,\n        callbacks = [\n            tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=81, restore_best_weights=True),\n            tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=10, factor=tf.math.exp(-0.1), min_lr=2e-5)\n        ]\n    ).history\n\n    # Plot results\n    plot_history(best_history)\n    plot_cm(best_model)","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:19:33.611241Z","iopub.execute_input":"2022-12-16T16:19:33.612133Z","iopub.status.idle":"2022-12-16T16:20:12.607986Z","shell.execute_reply.started":"2022-12-16T16:19:33.612090Z","shell.execute_reply":"2022-12-16T16:20:12.605780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Needed to write correct preprocessing code (meaningful only if the \"final_train\" flag is set,\n# otherwise simply returns the scaler fit on the last fold)\nprint('[', end = '')\nprint(*scaler.mean_, sep=',', end = '')\nprint(']')\nprint('[', end = '')\nprint(*scaler.scale_, sep=',', end = '')\nprint(']')\nprint('[', end = '')\nprint(*scaler.var_, sep=',', end = '')\nprint(']')","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:20:12.609341Z","iopub.status.idle":"2022-12-16T16:20:12.610383Z","shell.execute_reply.started":"2022-12-16T16:20:12.610110Z","shell.execute_reply":"2022-12-16T16:20:12.610147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save = True\n\nif save:\n    best_model.save('best_model', include_optimizer=False)\n    shutil.make_archive('best_model', 'zip', 'best_model')","metadata":{"execution":{"iopub.status.busy":"2022-12-16T16:20:12.612120Z","iopub.status.idle":"2022-12-16T16:20:12.612774Z","shell.execute_reply.started":"2022-12-16T16:20:12.612451Z","shell.execute_reply":"2022-12-16T16:20:12.612482Z"},"trusted":true},"execution_count":null,"outputs":[]}]}