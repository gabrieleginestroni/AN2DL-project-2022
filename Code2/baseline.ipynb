{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfehjCy896Fd"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_wVYNVVfr6q",
        "outputId": "812f6cdf-089b-4ef4-a5f6-7e69cdcfa205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.9.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', size=16) \n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQWEHP13tQsc"
      },
      "source": [
        "### Set seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "imBtfhUPLwB-"
      },
      "outputs": [],
      "source": [
        "# Random seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GMysAd_I-lED"
      },
      "source": [
        " # Load the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "C0m6Gn4fkm9S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2429, 36, 6)\n",
            "(2429,)\n"
          ]
        }
      ],
      "source": [
        "path = os.getcwd()\n",
        "# Load the .npy file\n",
        "X = np.load(path+r'\\training\\x_train.npy')\n",
        "y = np.load(path+r'\\training\\y_train.npy')\n",
        "print(X.shape)   #Note as sequences are already built, with window 36\n",
        "print(y.shape)\n",
        "print(X.dtype)\n",
        "print(y.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[ 1.7597e+01  8.1713e+00 -1.7842e+00 -1.9706e+01 -9.7135e+00\n",
            "   -6.1887e+01]\n",
            "  [ 2.2974e-01 -5.8956e+00  1.6837e+01  5.0390e+00  2.4332e+00\n",
            "    5.8914e+01]\n",
            "  [-2.9654e+01 -2.1296e+01  2.9103e+01 -4.7503e-01  7.5391e+01\n",
            "    1.0665e+01]\n",
            "  ...\n",
            "  [ 1.1982e+02  2.1849e+02  1.6398e+03  5.9787e+02  2.1922e+03\n",
            "    1.8484e+01]\n",
            "  [ 9.1591e+01  1.0378e+02  1.8238e+03  1.2460e+03  1.1301e+04\n",
            "    7.5107e+02]\n",
            "  [ 4.3719e+01  3.0689e+01  9.5092e+02  8.5927e+02  8.1331e+03\n",
            "    9.8883e+02]]\n",
            "\n",
            " [[ 2.4209e+00 -5.6039e-01  2.3654e+01 -3.8171e+00  1.3721e+00\n",
            "    8.2510e+00]\n",
            "  [ 1.6847e+00  2.3989e+01  1.7803e+01 -1.9575e+01 -8.2534e+00\n",
            "   -2.3959e+01]\n",
            "  [ 5.0692e-01  5.0281e+01  5.1196e+01  1.6036e+01  7.2974e-01\n",
            "   -1.8185e+01]\n",
            "  ...\n",
            "  [-1.5618e+01 -7.4233e+00  3.7833e+00  1.6565e+00 -1.0438e+01\n",
            "   -5.5229e+00]\n",
            "  [-1.0702e+01 -6.3568e+00 -4.6464e+00  6.7957e-01 -5.5260e+00\n",
            "    1.9070e+01]\n",
            "  [-4.6352e+00 -3.2769e+00 -5.9048e+00 -1.9564e-01  1.1487e+00\n",
            "    2.3275e+01]]\n",
            "\n",
            " [[ 3.9688e+01  7.0129e+02  3.0038e+03  3.4806e+01  6.5036e+01\n",
            "    7.7808e+01]\n",
            "  [ 1.6399e+02  1.2559e+03  3.3818e+03  2.3965e+03  5.1936e+01\n",
            "   -7.0134e+01]\n",
            "  [ 2.7545e+02  1.7005e+03  3.0922e+03  3.3084e+03  1.9858e+02\n",
            "    3.6742e+02]\n",
            "  ...\n",
            "  [ 3.1521e+01  9.1987e+00 -4.7701e+00  1.5335e+01  1.7212e+01\n",
            "    1.2384e+01]\n",
            "  [ 2.5363e+01  5.1979e+00  6.0153e+00 -6.8815e+00  1.0936e+01\n",
            "   -6.4854e+01]\n",
            "  [ 1.2557e+01  2.2734e+00  4.4831e+00 -1.2885e+01  1.0488e+01\n",
            "   -6.6960e+01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 5.0312e+02  1.6427e+03  1.6508e+03  1.5424e+03  1.4157e+03\n",
            "    1.3090e+03]\n",
            "  [ 3.1731e+02  2.0105e+03  2.1930e+03  2.1599e+03  2.0417e+03\n",
            "    1.9044e+03]\n",
            "  [ 2.4274e+02  1.4771e+03  1.8077e+03  1.8751e+03  1.8384e+03\n",
            "    1.7504e+03]\n",
            "  ...\n",
            "  [ 4.4821e-01  2.2123e-01  5.3692e+00  1.4156e+01  1.6963e+01\n",
            "    1.9934e+01]\n",
            "  [-7.4565e-01  1.8507e+00  5.6273e+00  1.3796e+01  1.6155e+01\n",
            "    2.7067e+01]\n",
            "  [-7.2203e-01  2.8672e+00  6.6811e+00  1.3440e+01  1.7874e+01\n",
            "    2.7193e+01]]\n",
            "\n",
            " [[ 1.7756e+01  2.0621e+00 -3.2330e+00  3.2775e-01 -1.5536e+01\n",
            "    9.1006e+00]\n",
            "  [ 2.6275e+01  2.0191e+00 -2.5783e+00 -1.6558e+00 -3.8543e+00\n",
            "   -5.7283e+01]\n",
            "  [ 3.1817e+01  1.6151e+00 -1.3073e+00 -3.9150e+00  1.5539e+01\n",
            "   -4.3464e+01]\n",
            "  ...\n",
            "  [ 8.9397e+00  8.3143e+00  2.8575e+01  4.1574e+01  8.0197e+01\n",
            "    6.3055e+01]\n",
            "  [ 1.0139e+01  5.4037e+00  2.5684e+01  3.6630e+01  5.1392e+01\n",
            "    1.5032e+01]\n",
            "  [ 5.9396e+00  2.3820e+00  1.3211e+01  1.8603e+01  3.3123e+01\n",
            "    4.4679e-01]]\n",
            "\n",
            " [[ 1.0391e+00 -1.2397e-01  6.9213e-01  4.2632e-01  8.7345e-01\n",
            "    6.6273e-01]\n",
            "  [-3.9896e+00  2.1111e+00  1.2686e+00 -5.9710e-01  3.9892e-01\n",
            "   -9.4295e-01]\n",
            "  [ 1.8169e-01  1.3814e+00  6.6310e-01  1.2138e-01  1.4185e+00\n",
            "    9.8861e+00]\n",
            "  ...\n",
            "  [ 2.0638e-01  1.0287e+00 -9.2512e-01 -7.1633e-01  1.7333e+00\n",
            "    4.7638e+00]\n",
            "  [ 4.1429e+00  1.0939e+00 -1.8921e-01  1.1410e+00  9.3441e-01\n",
            "    4.1742e+00]\n",
            "  [-1.5545e+00 -1.2121e+00  1.0705e+00  1.3272e+00  2.4010e+00\n",
            "    8.4550e+00]]]\n",
            "[ 0  0  0 ... 11 11 11]\n"
          ]
        }
      ],
      "source": [
        "# visualizing X and y\n",
        "print(X)\n",
        "print(y)     "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1943,)\n",
            "[3 9 6 ... 9 1 5]\n"
          ]
        }
      ],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "print(y_train.shape)\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float64\n"
          ]
        }
      ],
      "source": [
        "scaler = MinMaxScaler()\n",
        "# Fit the scaler to the training data and apply the scaling on the training data\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply the scaling on validation data\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1943, 12)\n",
            "(486, 12)\n"
          ]
        }
      ],
      "source": [
        "# Convert the sparse labels to categorical values\n",
        "y_train = tfk.utils.to_categorical(y_train)\n",
        "y_test = tfk.utils.to_categorical(y_test)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map classes STRINGS to integers\n",
        "label_mapping = {\n",
        "    'Wish': 0,\n",
        "    'Another': 1,\n",
        "    'Comfortably': 2,\n",
        "    'Money': 3,\n",
        "    'Breathe': 4,\n",
        "    'Time': 5,\n",
        "    'Brain': 6,\n",
        "    'Echoes': 7,\n",
        "    'Wearing': 8,\n",
        "    'Sorrow': 9,\n",
        "    'Hey': 10,\n",
        "    'Shine': 11,  \n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "classes = y_train.shape[-1]\n",
        "batch_size = 128\n",
        "epochs = 200"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Vanilla Long Short Term Memory (LSTM) Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_LSTM_classifier(input_shape, classes):\n",
        "    # Build the neural network layer by layer\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    # Feature extractor\n",
        "    lstm = tfkl.LSTM(128, return_sequences=True)(input_layer)    # return_sequences=True -> output = batch_size * timestamps * features\n",
        "                                                                 # return_sequences=False -> output = batch_size * 1 (just last timestamp) * features\n",
        "    lstm = tfkl.LSTM(128)(lstm)\n",
        "    dropout = tfkl.Dropout(.5, seed=seed)(lstm)\n",
        "\n",
        "    # Classifier\n",
        "    classifier = tfkl.Dense(128, activation='relu')(dropout)\n",
        "    output_layer = tfkl.Dense(classes, activation='softmax')(classifier)\n",
        "\n",
        "    # Connect input and output through the Model class\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
        "\n",
        "    # Return the model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_LSTM_classifier(input_shape, classes)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    x = X_train,\n",
        "    y = y_train,\n",
        "    batch_size = batch_size,\n",
        "    epochs = epochs,\n",
        "    validation_split=.1,\n",
        "    callbacks = [\n",
        "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True),\n",
        "        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=5, factor=0.5, min_lr=1e-5)\n",
        "    ]\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_epoch = np.argmax(history['val_accuracy'])\n",
        "plt.figure(figsize=(17,4))\n",
        "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.title('Categorical Crossentropy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(17,4))\n",
        "plt.plot(history['accuracy'], label='Training accuracy', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(history['val_accuracy'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(17,4))\n",
        "plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the test set with the LSTM\n",
        "predictions = model.predict(X_test)\n",
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "\n",
        "# Compute the classification metrics\n",
        "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "print('Accuracy:',accuracy.round(4))\n",
        "print('Precision:',precision.round(4))\n",
        "print('Recall:',recall.round(4))\n",
        "print('F1:',f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm.T, cmap='Blues', xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()))\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Bidirectional Long Short Term Memory (BiLSTM) Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_BiLSTM_classifier(input_shape, classes):\n",
        "    # Build the neural network layer by layer\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    # Feature extractor\n",
        "    bilstm = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(input_layer)\n",
        "    bilstm = tfkl.Bidirectional(tfkl.LSTM(128))(bilstm)\n",
        "    dropout = tfkl.Dropout(.5, seed=seed)(bilstm)\n",
        "\n",
        "    # Classifier\n",
        "    classifier = tfkl.Dense(128, activation='relu')(dropout)\n",
        "    output_layer = tfkl.Dense(classes, activation='softmax')(classifier)\n",
        "\n",
        "    # Connect input and output through the Model class\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
        "\n",
        "    # Return the model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_BiLSTM_classifier(input_shape, classes)\n",
        "model.summary() #note double number of parameters (this is because we have 2 lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    x = X_train,\n",
        "    y = y_train,\n",
        "    batch_size = batch_size,\n",
        "    epochs = epochs,\n",
        "    validation_split=.1,\n",
        "    callbacks = [\n",
        "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True),\n",
        "        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=5, factor=0.5, min_lr=1e-5)\n",
        "    ]\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_epoch = np.argmax(history['val_accuracy'])\n",
        "plt.figure(figsize=(17,4))\n",
        "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.title('Categorical Crossentropy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(17,4))\n",
        "plt.plot(history['accuracy'], label='Training accuracy', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(history['val_accuracy'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(18,3))\n",
        "plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the test set with the BiLSTM\n",
        "predictions = model.predict(X_test)\n",
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "\n",
        "# Compute the classification metrics\n",
        "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "print('Accuracy:',accuracy.round(4))\n",
        "print('Precision:',precision.round(4))\n",
        "print('Recall:',recall.round(4))\n",
        "print('F1:',f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm.T, cmap='Blues', xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()))\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1D Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_1DCNN_classifier(input_shape, classes):\n",
        "    # Build the neural network layer by layer\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "    # Feature extractor\n",
        "    cnn = tfkl.Conv1D(128,3,padding='same',activation='relu')(input_layer)\n",
        "    cnn = tfkl.MaxPooling1D()(cnn)\n",
        "    cnn = tfkl.Conv1D(128,3,padding='same',activation='relu')(cnn)\n",
        "    gap = tfkl.GlobalAveragePooling1D()(cnn)\n",
        "    dropout = tfkl.Dropout(.5, seed=seed)(gap)\n",
        "\n",
        "    # Classifier\n",
        "    classifier = tfkl.Dense(128, activation='relu')(dropout)\n",
        "    output_layer = tfkl.Dense(classes, activation='softmax')(classifier)\n",
        "\n",
        "    # Connect input and output through the Model class\n",
        "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
        "\n",
        "    # Return the model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_1DCNN_classifier(input_shape, classes)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    x = X_train,\n",
        "    y = y_train,\n",
        "    batch_size = batch_size,\n",
        "    epochs = epochs,\n",
        "    validation_split=.1,\n",
        "    callbacks = [\n",
        "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True),\n",
        "        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=5, factor=0.5, min_lr=1e-5)\n",
        "    ]\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_epoch = np.argmax(history['val_accuracy'])\n",
        "plt.figure(figsize=(17,4))\n",
        "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.title('Categorical Crossentropy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(17,4))\n",
        "plt.plot(history['accuracy'], label='Training accuracy', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(history['val_accuracy'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(18,3))\n",
        "plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the test set with the 1DCNN\n",
        "predictions = model.predict(X_test)\n",
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "\n",
        "# Compute the classification metrics\n",
        "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
        "print('Accuracy:',accuracy.round(4))\n",
        "print('Precision:',precision.round(4))\n",
        "print('Recall:',recall.round(4))\n",
        "print('F1:',f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm.T, cmap='Blues', xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()))\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "gputensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "11b78048da93356f505538a9c7d58128f235a1fdc0d1d6d5bf0f972a0c6e57fd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
