{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries imports and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version =  2.10.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score,\n",
    "                             precision_score, recall_score)\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(f\"Tensorflow version = \", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different directories whether the code is executed on Kaggle or on a local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dir = \"/kaggle/input/competition\"\n",
    "local_dir = os.getcwd() + '/training_data_final'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and automatically splitting in training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3542 files belonging to 8 classes.\n",
      "Using 2834 files for training.\n",
      "Found 3542 files belonging to 8 classes.\n",
      "Using 708 files for validation.\n",
      "labels =  ['Species1', 'Species2', 'Species3', 'Species4', 'Species5', 'Species6', 'Species7', 'Species8']\n"
     ]
    }
   ],
   "source": [
    "input_shape = (96, 96, 3)\n",
    "input_size = input_shape[:-1]\n",
    "batch_gen = 128\n",
    "dir = local_dir\n",
    "\n",
    "traininig_set = tfk.utils.image_dataset_from_directory(\n",
    "    directory=dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(96, 96),\n",
    "    batch_size=batch_gen)\n",
    "\n",
    "validation_set = tfk.utils.image_dataset_from_directory(\n",
    "    directory=dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(96, 96),\n",
    "    batch_size=batch_gen)\n",
    "\n",
    "labels = traininig_set.class_names\n",
    "print(\"labels = \", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation of the dataset in an online fashion as pre-processing layers to be inserted in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tfkl.Rescaling(1.0 / 255)\n",
    "\n",
    "translation_layer = tfkl.RandomTranslation(\n",
    "    height_factor=0.3,\n",
    "    width_factor=0.3,\n",
    "    fill_mode='reflect',\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "rotation_layer = tfkl.RandomRotation(\n",
    "    factor=0.3,\n",
    "    fill_mode='reflect',\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "zoom_layer = tfkl.RandomZoom(\n",
    "    height_factor=0.3,\n",
    "    width_factor=0.3,\n",
    "    fill_mode='reflect',\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "contrast_layer = tfkl.RandomContrast(factor=0.1, seed=seed)\n",
    "\n",
    "flip_layer = tfkl.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "\n",
    "brightness_layer = tfkl.RandomBrightness(factor=0.2, seed=seed)\n",
    "\n",
    "\n",
    "# Note: Data augmentation is inactive at test time so input images will only be augmented during calls to Model.fit\n",
    "# (not Model.evaluate or Model.predict).\n",
    "data_augmentation_preprocessing_layers = tfk.Sequential(\n",
    "    [normalization_layer, translation_layer, rotation_layer, zoom_layer, contrast_layer, brightness_layer, flip_layer])\n",
    "\n",
    "\n",
    "#In alternative: https://www.tensorflow.org/tutorials/images/data_augmentation#random_transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tests to do with the augmentation, to see results, and check what works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#layers.adapt method applies the modifications, useful to show the results of the augmentation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# https://www.tensorflow.org/guide/keras/preprocessing_layers#the_adapt_method\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m layer\u001b[39m.\u001b[39madapt(image)\n\u001b[0;32m      6\u001b[0m \u001b[39m# applies data augmentation to the training set before training\u001b[39;00m\n\u001b[0;32m      7\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mbatch(\u001b[39m16\u001b[39m)\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (data_augmentation(x), y))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layer' is not defined"
     ]
    }
   ],
   "source": [
    "#layers.adapt method applies the modifications, useful to show the results of the augmentation\n",
    "# https://www.tensorflow.org/guide/keras/preprocessing_layers#the_adapt_method\n",
    "#layer.adapt(image)\n",
    "\n",
    "\n",
    "# applies data augmentation to the training set before training\n",
    "#train_dataset = train_dataset.batch(16).map(lambda x, y: (data_augmentation(x), y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute class weights automatically (or not...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_numeric = []\\nfor v in y_val:\\n    y_numeric.append(np.argmax(v))\\n\\n\\nclass_weights = dict(enumerate(class_weight.compute_class_weight(\\n    \\'balanced\\', classes=labels, y=y_numeric)))\\nprint(class_weights)\\n\\n\\nprint(f\"training set input shape\", traininig_set.shape,)\\nprint(f\"validation set input shape\", validation_set.shape)\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the class weights in order to balance loss during training\n",
    "# TODO: I'll fix this later\n",
    "'''\n",
    "y_numeric = []\n",
    "for v in y_val:\n",
    "    y_numeric.append(np.argmax(v))\n",
    "\n",
    "\n",
    "class_weights = dict(enumerate(class_weight.compute_class_weight(\n",
    "    'balanced', classes=labels, y=y_numeric)))\n",
    "print(class_weights)\n",
    "\n",
    "\n",
    "print(f\"training set input shape\", traininig_set.shape,)\n",
    "print(f\"validation set input shape\", validation_set.shape)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = {0: \"Species1\", 1: \"Species2\", 2: \"Species3\", 3: \"Species4\",\n",
    "          4: \"Species5\", 5: \"Species6\", 6: \"Species7\", 7: \"Species8\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online augmentation with dataset mapping requires caching and prefetching in order to avoid bottlenecking the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_ds = traininig_set.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "#val_ds = validation_set.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stupid_model(input_shape):\n",
    "\ttf.random.set_seed(seed)\n",
    "\n",
    "\t# stupid model just to test this shit\n",
    "\n",
    "\tinput_layer = tfk.Input(shape=input_shape)\n",
    "\tx = data_augmentation_preprocessing_layers(input_layer)\n",
    "\n",
    "\tconvolution = tfk.Sequential([\n",
    "\t\ttfkl.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "\t\ttfkl.MaxPooling2D(),\n",
    "\t\ttfkl.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "\t\ttfkl.MaxPooling2D(),\n",
    "\t\ttfkl.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "\t\ttfkl.MaxPooling2D(),\n",
    "\t\ttfkl.Flatten(),\n",
    "\t\ttfkl.Dense(128, activation='relu'),\n",
    "\t])\n",
    "\n",
    "\tx = convolution(x)\n",
    "\toutput_layer = tfkl.Dense(\n",
    "\t\tunits=len(labels),\n",
    "\t\tactivation='softmax',\n",
    "\t\tkernel_initializer=tfk.initializers.GlorotUniform(seed),\n",
    "\t\tname='output_layer')(x)\n",
    "\n",
    "\t# Connect input and output through the Model class\n",
    "\tmodel = tfk.Model(inputs = input_layer, outputs = output_layer, name = 'stupidity')\n",
    "\n",
    "\tdef get_lr_metric(optimizer):\n",
    "\t\tdef lr(y_true, y_pred):\n",
    "\t\t\treturn optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr method because the later one is not working for me\n",
    "\t\treturn lr\n",
    "\toptimizer = tfk.optimizers.Adam()\n",
    "\tlr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "\t# Compile the model\n",
    "\tmodel.compile(loss=tfk.losses.SparseCategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy', lr_metric])\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"stupidity\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " sequential_6 (Sequential)   (None, 128)               2658176   \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,659,208\n",
      "Trainable params: 2,659,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().setLevel('ERROR') # stupid warnings that can be discarded\n",
    "\n",
    "model = build_stupid_model(input_shape)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      " 5/23 [=====>........................] - ETA: 6:37 - loss: 6.3138 - accuracy: 0.1297 - lr: 0.0010"
     ]
    }
   ],
   "source": [
    "# good GPU utilization on my machine with this batch size\n",
    "batch_size = 64\n",
    "epochs = 400\n",
    "\n",
    "# exponential decay for the learning rate\n",
    "learn_strating_rate = 1e-3 # suggested 5e-5 for transfer learning applications\n",
    "def scheduler(epoch, lr):\n",
    "    return learn_strating_rate * tf.math.exp(- epoch / 50.0)\n",
    "\n",
    "early_stop = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True)\n",
    "\n",
    "learning_rate_scheduler = tfk.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# training\n",
    "history = model.fit(\n",
    "    x=traininig_set,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=validation_set,\n",
    "    \n",
    "    # class_weight=class_weights, # TODO: I have to sleep\n",
    "    callbacks=[early_stop, learning_rate_scheduler]\n",
    ").history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['loss'], label='Std training',\n",
    "         alpha=.3, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(history['val_loss'], label='Std validation',\n",
    "         alpha=.8, color='#ff7f0e')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['accuracy'], label='Std training',\n",
    "         alpha=.8, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(history['val_accuracy'], label='Std validation',\n",
    "         alpha=.8, color='#ff7f0e')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix (evaluated on the validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_val)\n",
    "cm = confusion_matrix(np.argmax(y_val, axis=-1),\n",
    "                      np.argmax(predictions, axis=-1))\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_val, axis=-1),\n",
    "                          np.argmax(predictions, axis=-1))\n",
    "precision = precision_score(\n",
    "    np.argmax(y_val, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "recall = recall_score(np.argmax(y_val, axis=-1),\n",
    "                      np.argmax(predictions, axis=-1), average='macro')\n",
    "f1 = f1_score(np.argmax(y_val, axis=-1),\n",
    "              np.argmax(predictions, axis=-1), average='macro')\n",
    "print('Accuracy:', accuracy.round(4))\n",
    "print('Precision:', precision.round(4))\n",
    "print('Recall:', recall.round(4))\n",
    "print('F1:', f1.round(4))\n",
    "\n",
    "plt.figure(figsize=(6, 5.5))\n",
    "sns.heatmap(cm.T, xticklabels=labels, yticklabels=labels)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot one example of an image for each class from the validation set of images.\n",
    "For each image show the prediction on a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 2)\n",
    "fig.set_size_inches(15, 30)\n",
    "\n",
    "example_prediction = [0] * 8\n",
    "\n",
    "for i in range(8):\n",
    "    example_from_validation = -1\n",
    "\n",
    "    while example_from_validation == -1:\n",
    "        example_from_validation = random.choice(range(len(X_val)))\n",
    "        if np.argmax(y_val[example_from_validation]) == i:\n",
    "            example_prediction[i] = example_from_validation\n",
    "        else:\n",
    "            example_from_validation = -1\n",
    "\n",
    "    predicted = model.predict(\n",
    "        X_val[example_prediction[i]].reshape(1, 96, 96, 3))\n",
    "\n",
    "    axes[i, 0].imshow(X_val[example_prediction[i]])\n",
    "    axes[i, 0].set_title(\n",
    "        'True label: ' + labels[np.argmax(y_val[example_prediction[i]])])\n",
    "    axes[i, 1].barh(list(labels.values()), predicted[0],\n",
    "                    color=plt.get_cmap('Paired').colors)\n",
    "    axes[i, 1].set_title('Predicted label: ' + labels[np.argmax(predicted)])\n",
    "    axes[i, 1].grid(alpha=.3)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "Here it is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 - 2s - loss: 0.2023 - accuracy: 0.9229\n",
      "17/17 - 1s - loss: 1.0335 - accuracy: 0.6654\n",
      "No improvement!\n"
     ]
    }
   ],
   "source": [
    "restored_model = tfk.models.load_model('simo_model')\n",
    "\n",
    "# TODO: not right because validation set can change\n",
    "restored_loss, restored_acc = restored_model.evaluate(X_val, y_val, verbose=2)\n",
    "loss, acc = model.evaluate(X_val, y_val, verbose=2)\n",
    "if acc > restored_acc:  # know that this is conceptually wrong\n",
    "    print(\"Model improved!\")\n",
    "    model.save('simo_model')\n",
    "else:\n",
    "    print(\"No improvement!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('anndl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "830302ffa9751c9d00ec7acc7b0cf1db10db4d450fa3f08a82573b165c9771b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
