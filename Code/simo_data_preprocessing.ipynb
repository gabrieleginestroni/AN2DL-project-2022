{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries imports and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version =  2.10.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score,\n",
    "                             precision_score, recall_score)\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(f\"Tensorflow version = \", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different directories whether the code is executed on Kaggle or on a local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dir = \"/kaggle/input/competition\"\n",
    "local_dir = os.getcwd() + '/training_data_final'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and automatically splitting in training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3542 files belonging to 8 classes.\n",
      "Using 2834 files for training.\n",
      "Found 3542 files belonging to 8 classes.\n",
      "Using 708 files for validation.\n",
      "labels =  ['Species1', 'Species2', 'Species3', 'Species4', 'Species5', 'Species6', 'Species7', 'Species8']\n"
     ]
    }
   ],
   "source": [
    "input_shape = (96, 96, 3)\n",
    "input_size = input_shape[:-1]\n",
    "batch_gen = 16\n",
    "dir = local_dir\n",
    "\n",
    "traininig_set = tfk.utils.image_dataset_from_directory(\n",
    "    directory=dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(96, 96),\n",
    "    batch_size=batch_gen)\n",
    "\n",
    "validation_set = tfk.utils.image_dataset_from_directory(\n",
    "    directory=dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(96, 96),\n",
    "    batch_size=batch_gen)\n",
    "\n",
    "labels = traininig_set.class_names\n",
    "print(\"labels = \", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation of the dataset in an online fashion as pre-processing layers to be inserted in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tfkl.Rescaling(1.0 / 255)\n",
    "\n",
    "translation_layer = tfkl.RandomTranslation(\n",
    "    height_factor=0.3,\n",
    "    width_factor=0.3,\n",
    "    fill_mode='reflect',\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "rotation_layer = tfkl.RandomRotation(\n",
    "    factor=0.3,\n",
    "    fill_mode='reflect',\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "zoom_layer = tfkl.RandomZoom(\n",
    "    height_factor=0.3,\n",
    "    width_factor=0.3,\n",
    "    fill_mode='reflect',\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "contrast_layer = tfkl.RandomContrast(factor=0.1, seed=seed)\n",
    "\n",
    "flip_layer = tfkl.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "\n",
    "brightness_layer = tfkl.RandomBrightness(factor=0.2, seed=seed)\n",
    "\n",
    "\n",
    "# Note: Data augmentation is inactive at test time so input images will only be augmented during calls to Model.fit\n",
    "# (not Model.evaluate or Model.predict).\n",
    "data_augmentation_preprocessing_layers = tfk.Sequential(\n",
    "    [translation_layer, rotation_layer, zoom_layer, contrast_layer, brightness_layer, flip_layer])\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "#In alternative: https://www.tensorflow.org/tutorials/images/data_augmentation#random_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image, label = next(iter(traininig_set))\n",
    "#for i in range(8):\n",
    "\t#aug = data_augmentation_preprocessing_layers(image)[0]\n",
    "\t#plt.imshow(aug.numpy().astype('uint8'))\n",
    "\t#plt.title(label)\n",
    "\t#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tests to do with the augmentation, to see results, and check what works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers.adapt method applies the modifications, useful to show the results of the augmentation\n",
    "# https://www.tensorflow.org/guide/keras/preprocessing_layers#the_adapt_method\n",
    "#layer.adapt(image)\n",
    "\n",
    "\n",
    "# applies data augmentation to the training set before training\n",
    "#train_dataset = train_dataset.batch(16).map(lambda x, y: (data_augmentation(x), y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute class weights automatically (or not...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_numeric = []\\nfor v in y_val:\\n    y_numeric.append(np.argmax(v))\\n\\n\\nclass_weights = dict(enumerate(class_weight.compute_class_weight(\\n    \\'balanced\\', classes=labels, y=y_numeric)))\\nprint(class_weights)\\n\\n\\nprint(f\"training set input shape\", traininig_set.shape,)\\nprint(f\"validation set input shape\", validation_set.shape)\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the class weights in order to balance loss during training\n",
    "# TODO: I'll fix this later\n",
    "'''\n",
    "y_numeric = []\n",
    "for v in y_val:\n",
    "    y_numeric.append(np.argmax(v))\n",
    "\n",
    "\n",
    "class_weights = dict(enumerate(class_weight.compute_class_weight(\n",
    "    'balanced', classes=labels, y=y_numeric)))\n",
    "print(class_weights)\n",
    "\n",
    "\n",
    "print(f\"training set input shape\", traininig_set.shape,)\n",
    "print(f\"validation set input shape\", validation_set.shape)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = {0: \"Species1\", 1: \"Species2\", 2: \"Species3\", 3: \"Species4\",\n",
    "          4: \"Species5\", 5: \"Species6\", 6: \"Species7\", 7: \"Species8\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online augmentation with dataset mapping requires caching and prefetching in order to avoid bottlenecking the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# first epoch is going to be slow because of the data augmentation\n",
    "# successive epochs will be much faster thanks to the caching of the images\n",
    "traininig_set = traininig_set.map(lambda x, y: (data_augmentation_preprocessing_layers(x), y) )\n",
    "# \n",
    "\n",
    "# CPU caching and prefetching for speedup\n",
    "traininig_set = traininig_set.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_set = validation_set.prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stupid_model(input_shape):\n",
    "\ttf.random.set_seed(seed)\n",
    "\n",
    "\t# stupid model just to test this shit\n",
    "\n",
    "\tinput_layer = tfk.Input(shape=input_shape)\n",
    "\n",
    "\t# if data augmentation applied here, the training is super slow\n",
    "\tx = data_augmentation_preprocessing_layers(input_layer)\n",
    "\n",
    "\tconvolution = tfk.Sequential([\n",
    "\t\ttfkl.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "\t\ttfkl.MaxPooling2D(),\n",
    "\t\ttfkl.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "\t\ttfkl.MaxPooling2D(),\n",
    "\t\ttfkl.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "\t\ttfkl.MaxPooling2D(),\n",
    "\t\ttfkl.Flatten(),\n",
    "\t\ttfkl.Dense(128, activation='relu'),\n",
    "\t\ttfkl.Dense(128, activation='relu'),\n",
    "\t\ttfkl.Dense(128, activation='relu')\n",
    "\t])\n",
    "\n",
    "\tx = convolution(x)\n",
    "\toutput_layer = tfkl.Dense(\n",
    "\t\tunits=len(labels),\n",
    "\t\tactivation='softmax',\n",
    "\t\tkernel_initializer=tfk.initializers.GlorotUniform(seed),\n",
    "\t\tname='output_layer')(x)\n",
    "\n",
    "\t# Connect input and output through the Model class\n",
    "\tmodel = tfk.Model(inputs = input_layer, outputs = output_layer, name = 'stupidity')\n",
    "\n",
    "\tdef get_lr_metric(optimizer):\n",
    "\t\tdef lr(y_true, y_pred):\n",
    "\t\t\treturn optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr method because the later one is not working for me\n",
    "\t\treturn lr\n",
    "\toptimizer = tfk.optimizers.Adam()\n",
    "\tlr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "\t# Compile the model\n",
    "\tmodel.compile(loss=tfk.losses.SparseCategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy', lr_metric])\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"stupidity\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 128)               2691200   \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,692,232\n",
      "Trainable params: 2,692,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_stupid_model(input_shape)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "178/178 [==============================] - 177s 794ms/step - loss: 2.7384 - accuracy: 0.1440 - lr: 0.0010 - val_loss: 2.0451 - val_accuracy: 0.1285 - val_lr: 1.0000e-03\n",
      "Epoch 2/400\n",
      "178/178 [==============================] - 143s 801ms/step - loss: 2.0287 - accuracy: 0.1796 - lr: 0.0010 - val_loss: 2.0610 - val_accuracy: 0.1328 - val_lr: 1.0000e-03\n",
      "Epoch 3/400\n",
      "178/178 [==============================] - 137s 767ms/step - loss: 2.0134 - accuracy: 0.1803 - lr: 0.0010 - val_loss: 1.9605 - val_accuracy: 0.2684 - val_lr: 1.0000e-03\n",
      "Epoch 4/400\n",
      "178/178 [==============================] - 142s 795ms/step - loss: 2.0098 - accuracy: 0.1948 - lr: 0.0010 - val_loss: 2.0057 - val_accuracy: 0.1582 - val_lr: 1.0000e-03\n",
      "Epoch 5/400\n",
      " 49/178 [=======>......................] - ETA: 1:40 - loss: 1.9882 - accuracy: 0.2054 - lr: 1.0000e-03"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [35], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m early_stop \u001b[39m=\u001b[39m tfk\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(\n\u001b[0;32m     13\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[39m# training\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     18\u001b[0m     x\u001b[39m=\u001b[39;49mtraininig_set,\n\u001b[0;32m     19\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     20\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     21\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_set,\n\u001b[0;32m     22\u001b[0m     \n\u001b[0;32m     23\u001b[0m     \u001b[39m# class_weight=class_weights, # TODO: I have to sleep\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stop]\n\u001b[0;32m     25\u001b[0m )\u001b[39m.\u001b[39mhistory\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\anndl\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\anndl\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\anndl\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\anndl\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\anndl\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\anndl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\anndl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\anndl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\anndl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# good GPU utilization on my machine with this batch size\n",
    "batch_size = 128\n",
    "epochs = 400\n",
    "\n",
    "# exponential decay for the learning rate\n",
    "learn_strating_rate = 1e-3 # suggested 5e-5 for transfer learning applications\n",
    "def scheduler(epoch, lr):\n",
    "    return learn_strating_rate * tf.math.exp(- epoch / 50.0)\n",
    "\n",
    "learning_rate_scheduler = tfk.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "early_stop = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# training\n",
    "history = model.fit(\n",
    "    x=traininig_set,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=validation_set,\n",
    "    \n",
    "    # class_weight=class_weights, # TODO: I have to sleep\n",
    "    callbacks=[early_stop]\n",
    ").history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['loss'], label='Std training',\n",
    "         alpha=.3, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(history['val_loss'], label='Std validation',\n",
    "         alpha=.8, color='#ff7f0e')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['accuracy'], label='Std training',\n",
    "         alpha=.8, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(history['val_accuracy'], label='Std validation',\n",
    "         alpha=.8, color='#ff7f0e')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix (evaluated on the validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_val)\n",
    "cm = confusion_matrix(np.argmax(y_val, axis=-1),\n",
    "                      np.argmax(predictions, axis=-1))\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_val, axis=-1),\n",
    "                          np.argmax(predictions, axis=-1))\n",
    "precision = precision_score(\n",
    "    np.argmax(y_val, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "recall = recall_score(np.argmax(y_val, axis=-1),\n",
    "                      np.argmax(predictions, axis=-1), average='macro')\n",
    "f1 = f1_score(np.argmax(y_val, axis=-1),\n",
    "              np.argmax(predictions, axis=-1), average='macro')\n",
    "print('Accuracy:', accuracy.round(4))\n",
    "print('Precision:', precision.round(4))\n",
    "print('Recall:', recall.round(4))\n",
    "print('F1:', f1.round(4))\n",
    "\n",
    "plt.figure(figsize=(6, 5.5))\n",
    "sns.heatmap(cm.T, xticklabels=labels, yticklabels=labels)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot one example of an image for each class from the validation set of images.\n",
    "For each image show the prediction on a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 2)\n",
    "fig.set_size_inches(15, 30)\n",
    "\n",
    "example_prediction = [0] * 8\n",
    "\n",
    "for i in range(8):\n",
    "    example_from_validation = -1\n",
    "\n",
    "    while example_from_validation == -1:\n",
    "        example_from_validation = random.choice(range(len(X_val)))\n",
    "        if np.argmax(y_val[example_from_validation]) == i:\n",
    "            example_prediction[i] = example_from_validation\n",
    "        else:\n",
    "            example_from_validation = -1\n",
    "\n",
    "    predicted = model.predict(\n",
    "        X_val[example_prediction[i]].reshape(1, 96, 96, 3))\n",
    "\n",
    "    axes[i, 0].imshow(X_val[example_prediction[i]])\n",
    "    axes[i, 0].set_title(\n",
    "        'True label: ' + labels[np.argmax(y_val[example_prediction[i]])])\n",
    "    axes[i, 1].barh(list(labels.values()), predicted[0],\n",
    "                    color=plt.get_cmap('Paired').colors)\n",
    "    axes[i, 1].set_title('Predicted label: ' + labels[np.argmax(predicted)])\n",
    "    axes[i, 1].grid(alpha=.3)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "Here it is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 - 2s - loss: 0.2023 - accuracy: 0.9229\n",
      "17/17 - 1s - loss: 1.0335 - accuracy: 0.6654\n",
      "No improvement!\n"
     ]
    }
   ],
   "source": [
    "restored_model = tfk.models.load_model('simo_model')\n",
    "\n",
    "# TODO: not right because validation set can change\n",
    "restored_loss, restored_acc = restored_model.evaluate(X_val, y_val, verbose=2)\n",
    "loss, acc = model.evaluate(X_val, y_val, verbose=2)\n",
    "if acc > restored_acc:  # know that this is conceptually wrong\n",
    "    print(\"Model improved!\")\n",
    "    model.save('simo_model')\n",
    "else:\n",
    "    print(\"No improvement!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('anndl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "830302ffa9751c9d00ec7acc7b0cf1db10db4d450fa3f08a82573b165c9771b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
